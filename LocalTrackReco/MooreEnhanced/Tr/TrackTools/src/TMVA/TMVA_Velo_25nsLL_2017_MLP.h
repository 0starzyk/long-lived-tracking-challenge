/*****************************************************************************\
* (c) Copyright 2019 CERN for the benefit of the LHCb Collaboration           *
*                                                                             *
* This software is distributed under the terms of the GNU General Public      *
* Licence version 3 (GPL Version 3), copied verbatim in the file "COPYING".   *
*                                                                             *
* In applying this licence, CERN does not waive the privileges and immunities *
* granted to it by virtue of its status as an Intergovernmental Organization  *
* or submit itself to any jurisdiction.                                       *
\*****************************************************************************/
// Class: ReadMLP_1
// Automatically generated by MethodBase::MakeClass
//

/* configuration options =====================================================

#GEN -*-*-*-*-*-*-*-*-*-*-*- general info -*-*-*-*-*-*-*-*-*-*-*-

Method         : MLP::MLP
TMVA Release   : 4.2.1         [262657]
ROOT Release   : 6.06/08       [394760]
Creator        : mexu
Date           : Thu Jan 31 11:01:13 2019
Host           : Linux lcgapp-slc6-x86-64-2.cern.ch 2.6.32-504.1.3.el6.x86_64 #1 SMP Wed Nov 12 06:58:35 CET 2014 x86_64
x86_64 x86_64 GNU/Linux Dir            : /afs/cern.ch/work/m/mexu/workspace/Ghost_Study/Final_Verstion Training events:
490649 Analysis type  : [Classification]


#OPT -*-*-*-*-*-*-*-*-*-*-*-*- options -*-*-*-*-*-*-*-*-*-*-*-*-

# Set by User:
NCycles: "600" [Number of training cycles]
HiddenLayers: "N+5" [Specification of hidden layer architecture]
NeuronType: "ReLU" [Neuron activation function type]
EstimatorType: "CE" [MSE (Mean Square Estimator) for Gaussian Likelihood or CE(Cross-Entropy) for Bernoulli Likelihood]
V: "False" [Verbose output (short form of "VerbosityLevel" below - overrides the latter one)]
VarTransform: "N" [List of variable transformations performed before training, e.g.,
"D_Background,P_Signal,G,N_AllClasses" for: "Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for
the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is
assumed)"] H: "False" [Print method-specific help message] CreateMVAPdfs: "True" [Create PDFs for classifier outputs
(signal and background)] TestRate: "5" [Test for overtraining performed at each #th epochs] UseRegulator: "False" [Use
regulator to avoid over-training] # Default: RandomSeed: "1" [Random seed for initial synapse weights (0 means unique
seed for each run; default value '1')] NeuronInputType: "sum" [Neuron input function type] VerbosityLevel: "Default"
[Verbosity level] IgnoreNegWeightsInTraining: "False" [Events with negative weights are ignored in the training (but are
included for testing and performance evaluation)] TrainingMethod: "BP" [Train with Back-Propagation (BP), BFGS Algorithm
(BFGS), or Genetic Algorithm (GA - slower and worse)] LearningRate: "2.000000e-02" [ANN learning rate parameter]
DecayRate: "1.000000e-02" [Decay rate for learning parameter]
EpochMonitoring: "False" [Provide epoch-wise monitoring plots according to TestRate (caution: causes big ROOT output
file!)] Sampling: "1.000000e+00" [Only 'Sampling' (randomly selected) events are trained each epoch] SamplingEpoch:
"1.000000e+00" [Sampling is used for the first 'SamplingEpoch' epochs, afterwards, all events are taken for training]
SamplingImportance: "1.000000e+00" [ The sampling weights of events in epochs which successful (worse estimator than
before) are multiplied with SamplingImportance, else they are divided.] SamplingTraining: "True" [The training sample is
sampled] SamplingTesting: "False" [The testing sample is sampled] ResetStep: "50" [How often BFGS should reset history]
Tau: "3.000000e+00" [LineSearch "size step"]
BPMode: "sequential" [Back-propagation learning mode: sequential or batch]
BatchSize: "-1" [Batch size: number of events/batch, only set if in Batch Mode, -1 for BatchSize=number_of_events]
ConvergenceImprove: "1.000000e-30" [Minimum improvement which counts as improvement (<0 means automatic convergence
check is turned off)] ConvergenceTests: "-1" [Number of steps (without improvement) required for convergence (<0 means
automatic convergence check is turned off)] UpdateLimit: "10000" [Maximum times of regulator update] CalculateErrors:
"False" [Calculates inverse Hessian matrix at the end of the training to be able to calculate the uncertainties of an
MVA value] WeightRange: "1.000000e+00" [Take the events for the estimator calculations from small deviations from the
desired value to large deviations only over the weight range]
##


#VAR -*-*-*-*-*-*-*-*-*-*-*-* variables *-*-*-*-*-*-*-*-*-*-*-*-

NVar 8
UpgradeGhostInfo_obsVP        UpgradeGhostInfo_obsVP        UpgradeGhostInfo_obsVP        UpgradeGhostInfo_obsVP 'F'
[3,21] UpgradeGhostInfo_FitVeloChi2  UpgradeGhostInfo_FitVeloChi2  UpgradeGhostInfo_FitVeloChi2
UpgradeGhostInfo_FitVeloChi2                                    'F'    [0,6727.17871094] UpgradeGhostInfo_FitVeloNDoF
UpgradeGhostInfo_FitVeloNDoF  UpgradeGhostInfo_FitVeloNDoF  UpgradeGhostInfo_FitVeloNDoF 'F'    [1,38]
UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits 'F'
[212,8281] UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits UpgradeGhostInfo_utHits
'F'    [155,5250] TRACK_CHI2                    TRACK_CHI2                    TRACK_CHI2                    TRACK_CHI2
'F'    [0,6727.17871094] TRACK_NDOF                    TRACK_NDOF                    TRACK_NDOF TRACK_NDOF 'F'    [1,38]
TRACK_ETA                     TRACK_ETA                     TRACK_ETA                     TRACK_ETA 'F'
[1.30405259132,8.10310745239] NSpec 0


============================================================================ */
#include "Kernel/STLExtensions.h"
#include "Kernel/TMV_utils.h"
#include "vdt/exp.h"
#include <array>
#include <string_view>

namespace Data::ReadGhostProbabilityVelo {
  namespace {
    constexpr auto ActivationFnc       = []( float x ) { return x > 0 ? x : 0; };
    constexpr auto OutputActivationFnc = []( float x ) {
      // sigmoid
      return 1.f / ( 1.f + vdt::fast_expf( -x ) );
    };
    // build network structure
    // weight matrix from layer 0 to 1
    constexpr auto fWeightMatrix0to1 = std::array<std::array<float, 9>, 13>{
        {{0.155915950692285, -0.339515556827506, 1.9446268750464, 1.6463940336952, -1.30672558653124, -0.24400698959765,
          1.43273437657558, 0.520115761598281, -1.12129823225399},
         {11.8336077337112, -1.04950031871576, -4.18127787878229, -0.602283592961662, 0.93409764117648,
          -0.29908170202171, -2.6034668257611, 3.6264981899805, 4.36152037430381},
         {0.0196953087545116, 2.18262450150064, 1.61091031602013, 0.117044503900492, -0.327278335267418,
          0.293844364332026, 5.45680428205418, -14.3860104397474, -4.3647268728156},
         {2.31969951405012, -0.700176000820744, 0.711792210836196, 1.74850167443025, -1.32342965617437,
          0.480262594058973, 1.21667948064601, 0.407188428527836, -1.08304006945441},
         {-0.784407284405185, 2.08371628883361, 0.960009276952923, 1.02859049699059, -1.56696642648604,
          -0.499921436491639, 1.27080185906256, 2.9442184548954, 1.46294239030415},
         {-7.45774813409986, -1.84138591365818, 7.58653819290778, -0.329164948295136, 0.139025894396367,
          -2.73458981345718, 5.12467129723733, -0.39801428468732, -0.0924650753302513},
         {-0.819520620843021, 0.566329610711499, 2.60089643122459, -3.03207214851132, 0.695872718553056,
          -2.25043216452866, 2.05345318289316, -4.4912968801099, -1.75983261695102},
         {3.77099116433501, 0.943064253815207, 0.789797890255954, 0.31053249873713, 0.0141422303620535,
          2.01228964472815, 0.745516656222548, -5.04479604089495, 0.575138484488292},
         {-7.73062893354229, -2.15654765068524, 3.84786842503382, 0.104337404823658, 0.118341269093766,
          -0.276383522074243, 4.77022644431208, 1.53141341150379, -1.34206167214305},
         {-5.26348971340225, 5.74870004805482, -5.05415216109238, 0.398858014539513, -0.208955466840829,
          6.34632047705806, -5.88844824789561, -0.0751188053775659, -1.86165303677131},
         {-6.64090090641417, 7.50203033989298, -7.83250881421125, 0.87238468900408, 0.0755699465123325,
          7.05398795136871, -7.98115251368685, -0.750236777166482, -5.59437635276389},
         {-0.103818430948369, 1.00204175828253, 1.8523912453487, 2.00916946020251, 0.976916781912148, 0.257385906743719,
          -1.61066323747631, 1.18128235409976, -0.359438144407157},
         {-5.61969721580479, -1.41341825810943, 3.29486537757352, 0.000326764244923571, -0.183662370179012,
          -3.05054433020513, 4.29403354260037, 2.09317424432339, -1.77506484791306}}};

    constexpr auto fWeightMatrix1to2 = std::array<float, 14>{
        {-1.85266936050757, -1.39395387656792, -4.82142963338587, -2.41429266412939, 2.5301608710542, 1.36426537323593,
         0.50922416373583, 4.87073341505186, 1.72233923244178, -1.14622976101016, -0.661445460193231,
         -0.880364573885965, 0.472109725834701, 5.99102841355357}};

    constexpr auto fMin = std::array<float, 8>{{3, 0, 1, 212, 155, 0, 1, 1.30405259132}};

    constexpr auto fMax = std::array<float, 8>{{21, 6727.17871094, 38, 8281, 5250, 6727.17871094, 38, 8.10310745239}};

    // Normalization transformation
    constexpr auto transformer = TMV::Utils::Transformer{fMin, fMax};

    // the training input variables
    constexpr auto validator = TMV::Utils::Validator{
        "ReadGhostProbabilityVelo",
        std::tuple{"UpgradeGhostInfo_obsVP", "UpgradeGhostInfo_FitVeloChi2", "UpgradeGhostInfo_FitVeloNDoF",
                   "UpgradeGhostInfo_veloHits", "UpgradeGhostInfo_utHits", "TRACK_CHI2", "TRACK_NDOF", "TRACK_ETA"}};

    constexpr auto l0To1 = TMV::Utils::Layer{fWeightMatrix0to1, ActivationFnc};
    constexpr auto l1To2 = TMV::Utils::Layer{fWeightMatrix1to2, OutputActivationFnc};
    constexpr auto MVA   = TMV::Utils::MVA{validator, transformer, 0, l0To1, l1To2};
  } // namespace
} // namespace Data::ReadGhostProbabilityVelo

//_______________________________________________________________________

struct ReadGhostProbabilityVelo final {

  // constructor
  ReadGhostProbabilityVelo( LHCb::span<const std::string_view, 8> theInputVars ) {
    Data::ReadGhostProbabilityVelo::MVA.validate( theInputVars );
  }

  // the classifier response
  // "inputValues" is a vector of input values in the same order as the
  // variables given to the constructor
  static constexpr auto GetMvaValue( LHCb::span<const float, 8> input ) {
    return Data::ReadGhostProbabilityVelo::MVA( input );
  }
};
