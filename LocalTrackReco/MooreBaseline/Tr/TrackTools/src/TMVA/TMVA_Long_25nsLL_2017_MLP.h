/*****************************************************************************\
* (c) Copyright 2019 CERN for the benefit of the LHCb Collaboration           *
*                                                                             *
* This software is distributed under the terms of the GNU General Public      *
* Licence version 3 (GPL Version 3), copied verbatim in the file "COPYING".   *
*                                                                             *
* In applying this licence, CERN does not waive the privileges and immunities *
* granted to it by virtue of its status as an Intergovernmental Organization  *
* or submit itself to any jurisdiction.                                       *
\*****************************************************************************/
// Class: ReadMLP_3
// Automatically generated by MethodBase::MakeClass
//

/* configuration options =====================================================

#GEN -*-*-*-*-*-*-*-*-*-*-*- general info -*-*-*-*-*-*-*-*-*-*-*-

Method         : MLP::MLP
TMVA Release   : 4.2.1         [262657]
ROOT Release   : 6.06/08       [394760]
Creator        : mexu
Date           : Thu Jan 31 10:47:51 2019
Host           : Linux lcgapp-slc6-x86-64-2.cern.ch 2.6.32-504.1.3.el6.x86_64 #1 SMP Wed Nov 12 06:58:35 CET 2014 x86_64
x86_64 x86_64 GNU/Linux Dir            : /afs/cern.ch/work/m/mexu/workspace/Ghost_Study/Final_Verstion Training events:
240898 Analysis type  : [Classification]


#OPT -*-*-*-*-*-*-*-*-*-*-*-*- options -*-*-*-*-*-*-*-*-*-*-*-*-

# Set by User:
NCycles: "600" [Number of training cycles]
HiddenLayers: "N+5" [Specification of hidden layer architecture]
NeuronType: "ReLU" [Neuron activation function type]
EstimatorType: "CE" [MSE (Mean Square Estimator) for Gaussian Likelihood or CE(Cross-Entropy) for Bernoulli Likelihood]
V: "False" [Verbose output (short form of "VerbosityLevel" below - overrides the latter one)]
VarTransform: "N" [List of variable transformations performed before training, e.g.,
"D_Background,P_Signal,G,N_AllClasses" for: "Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for
the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is
assumed)"] H: "False" [Print method-specific help message] CreateMVAPdfs: "True" [Create PDFs for classifier outputs
(signal and background)] TestRate: "5" [Test for overtraining performed at each #th epochs] UseRegulator: "False" [Use
regulator to avoid over-training] # Default: RandomSeed: "1" [Random seed for initial synapse weights (0 means unique
seed for each run; default value '1')] NeuronInputType: "sum" [Neuron input function type] VerbosityLevel: "Default"
[Verbosity level] IgnoreNegWeightsInTraining: "False" [Events with negative weights are ignored in the training (but are
included for testing and performance evaluation)] TrainingMethod: "BP" [Train with Back-Propagation (BP), BFGS Algorithm
(BFGS), or Genetic Algorithm (GA - slower and worse)] LearningRate: "2.000000e-02" [ANN learning rate parameter]
DecayRate: "1.000000e-02" [Decay rate for learning parameter]
EpochMonitoring: "False" [Provide epoch-wise monitoring plots according to TestRate (caution: causes big ROOT output
file!)] Sampling: "1.000000e+00" [Only 'Sampling' (randomly selected) events are trained each epoch] SamplingEpoch:
"1.000000e+00" [Sampling is used for the first 'SamplingEpoch' epochs, afterwards, all events are taken for training]
SamplingImportance: "1.000000e+00" [ The sampling weights of events in epochs which successful (worse estimator than
before) are multiplied with SamplingImportance, else they are divided.] SamplingTraining: "True" [The training sample is
sampled] SamplingTesting: "False" [The testing sample is sampled] ResetStep: "50" [How often BFGS should reset history]
Tau: "3.000000e+00" [LineSearch "size step"]
BPMode: "sequential" [Back-propagation learning mode: sequential or batch]
BatchSize: "-1" [Batch size: number of events/batch, only set if in Batch Mode, -1 for BatchSize=number_of_events]
ConvergenceImprove: "1.000000e-30" [Minimum improvement which counts as improvement (<0 means automatic convergence
check is turned off)] ConvergenceTests: "-1" [Number of steps (without improvement) required for convergence (<0 means
automatic convergence check is turned off)] UpdateLimit: "10000" [Maximum times of regulator update] CalculateErrors:
"False" [Calculates inverse Hessian matrix at the end of the training to be able to calculate the uncertainties of an
MVA value] WeightRange: "1.000000e+00" [Take the events for the estimator calculations from small deviations from the
desired value to large deviations only over the weight range]
##


#VAR -*-*-*-*-*-*-*-*-*-*-*-* variables *-*-*-*-*-*-*-*-*-*-*-*-

NVar 14
UpgradeGhostInfo_obsVP        UpgradeGhostInfo_obsVP        UpgradeGhostInfo_obsVP        UpgradeGhostInfo_obsVP 'F'
[3,21] UpgradeGhostInfo_FitVeloChi2  UpgradeGhostInfo_FitVeloChi2  UpgradeGhostInfo_FitVeloChi2
UpgradeGhostInfo_FitVeloChi2                                    'F'    [2.7475274833e-10,102.336967468]
UpgradeGhostInfo_FitVeloNDoF  UpgradeGhostInfo_FitVeloNDoF  UpgradeGhostInfo_FitVeloNDoF  UpgradeGhostInfo_FitVeloNDoF
'F'    [-1,37] UpgradeGhostInfo_obsFT        UpgradeGhostInfo_obsFT        UpgradeGhostInfo_obsFT UpgradeGhostInfo_obsFT
'F'    [9,15] UpgradeGhostInfo_FitTChi2     UpgradeGhostInfo_FitTChi2     UpgradeGhostInfo_FitTChi2
UpgradeGhostInfo_FitTChi2                                       'F'    [0.0102619789541,66.5717773438]
UpgradeGhostInfo_FitTNDoF     UpgradeGhostInfo_FitTNDoF     UpgradeGhostInfo_FitTNDoF     UpgradeGhostInfo_FitTNDoF 'F'
[2,9] UpgradeGhostInfo_obsUT        UpgradeGhostInfo_obsUT        UpgradeGhostInfo_obsUT        UpgradeGhostInfo_obsUT
'F'    [0,9] UpgradeGhostInfo_FitMatchChi2 UpgradeGhostInfo_FitMatchChi2 UpgradeGhostInfo_FitMatchChi2
UpgradeGhostInfo_FitMatchChi2                                   'F'    [0.00715538021177,105.609642029]
UpgradeGhostInfo_UToutlier    UpgradeGhostInfo_UToutlier    UpgradeGhostInfo_UToutlier    UpgradeGhostInfo_UToutlier 'F'
[0,2] UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits UpgradeGhostInfo_veloHits
'F'    [212,8281] UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits
UpgradeGhostInfo_utHits                                         'F'    [155,5250] TRACK_CHI2 TRACK_CHI2 TRACK_CHI2
TRACK_CHI2                                                      'F'    [1.11951160431,137.525756836] TRACK_PT TRACK_PT
TRACK_PT                      TRACK_PT                                                        'F' [7.11249446869,59177]
TRACK_ETA                     TRACK_ETA                     TRACK_ETA                     TRACK_ETA 'F'
[1.44909417629,7.59725427628] NSpec 0


============================================================================ */
#include "Kernel/STLExtensions.h"
#include "Kernel/TMV_utils.h"
#include "LHCbMath/SIMDWrapper.h"
#include "vdt/exp.h"
#include <array>
#include <string_view>

namespace Data::ReadGhostProbabilityLong {
  namespace {
    constexpr auto ActivationFnc       = []( float x ) { return x > 0 ? x : 0; };
    constexpr auto OutputActivationFnc = []( float x ) {
      // sigmoid
      return 1.f / ( 1.f + vdt::fast_expf( -x ) );
    };
    // build network structure
    // weight matrix from layer 0 to 1
    constexpr auto fWeightMatrix0to1 = std::array<std::array<float, 15>, 19>{
        {{5.84008864215784, 5.7132089325513, -11.774076877463, -0.525197364638224, -2.10752248935081, 1.0512927544685,
          -1.12155991776652, -2.58561628462317, 0.0992228738628284, 0.192528704813822, -0.613390855819448,
          4.25296092219979, -4.0921822291027, -2.18134746524848, -4.73754284089887},
         {1.66344577028159, -0.598750169887754, 0.0531064861828303, 2.5607968213282, 0.525580870260857,
          -1.98140273392573, 0.828298628926364, -2.35249389404903, -0.0936456892233266, 0.335888476719476,
          1.5111911155181, -1.79209755054697, 3.46195927614102, 1.11604548426141, 2.57515937324644},
         {0.976570617100278, -3.16684721060622, 1.57983046423539, 1.33656018756382, 0.886593158517521,
          -1.18598468775949, -0.41596809068519, 0.71431824067017, -0.103175380396077, -0.801818421906416,
          1.30790765525069, 0.444647614098278, 1.63044529621821, 2.37007819077581, 1.11409497331471},
         {0.116536766143438, 1.54795212694239, 0.0695824693968735, 0.208017895259441, 1.65937413919279,
          -1.44953317451649, -0.242459803107375, 1.47301156260766, 0.0466051616587633, 0.00901596977910132,
          0.433305324045754, -1.90293100878341, 12.3006694188548, -0.324342889898116, 14.2703179828866},
         {-1.6997392309673, 0.448738099146611, 2.14587034998245, 0.209490751910712, 1.97385174430359,
          -0.0972018416420203, 0.865232596768773, -1.83802977318431, 0.957658507607471, -1.4998284735707,
          0.780286490620671, -0.902188951719858, 1.26419819636829, -1.28593619528937, -1.30369549318148},
         {-4.47513900959089, -1.21729014870209, 5.23334966030565, 0.088398409664505, 0.00225196974705713,
          -0.001389421729015, -0.0377832658135079, -0.14483528157748, -0.0308937497828883, -0.0155651050032303,
          0.0134404006598958, -0.0481298945225874, -27.8015919778056, 0.0796592317283468, -28.8132872045372},
         {-2.78088770733935, -0.536936377945378, 2.99674509830482, 0.152143567054464, 1.60693706947471,
          -0.409181833072409, -5.77978162309887, -1.41678573352559, 0.964025193751181, -0.191405590028037,
          0.297720249459772, -2.72490134736942, 5.67146178976156, 6.72631975605295, -0.0774788261740297},
         {0.797400142896525, 1.45265645325635, -4.68296960180083, 2.01415787949434, -1.24503846305576, 1.2517635707848,
          0.306072577104065, -0.19169575691868, -0.485203898587955, -0.640964698631725, 0.76365160189993,
          3.07798554975012, 0.821565504655852, 1.5067495044986, -1.50641497082028},
         {-3.76266910196979, 1.3527908490411, 3.7982843206407, -0.954977456442108, 0.154079011376122, 0.451660725600022,
          0.563631877959229, 0.120010269306012, 0.0302669653501486, -0.778129210958202, -1.20657545449406,
          -3.26332348039989, -0.812713348885926, -5.09872763664646, -4.26278485952133},
         {-0.891006237320396, 3.21575381288335, -2.24635176843384, -0.247693350340371, -1.59624563981453,
          0.354935509408721, -0.347741452370358, -4.48367211243014, -0.694150402088191, 0.199192347506536,
          -0.950437786124135, 3.26410171162224, 1.80816358165691, 1.4345313542488, -2.27010193565675},
         {-6.58083150228792, 4.57742466870003, 8.11244037620293, 0.234541457606346, 3.655295735286, 0.211978858155832,
          2.00023314530108, 2.33226021170402, -0.98349797591006, -0.393980494614482, 0.233512666745326,
          -7.33221512173259, 15.1700585810683, 0.477217803909146, 21.0362736802595},
         {-4.57379367925932, 1.63245108273915, 6.00012164947578, 0.0586819505705027, 2.12413427167368,
          -0.0187961900816576, 0.923850304630178, 1.13055498185848, -0.28926191505795, 0.0597321839446547,
          -0.21711806049803, -4.6627399424678, 13.3369965251736, 1.92163531872045, 15.0570073119617},
         {-3.51244216124008, 2.2588940339985, 3.74940326566168, -0.519008552242696, 1.26346291671264, 0.22531238726144,
          1.24658912650967, 0.883188713504475, -0.0798116091124166, -0.555412065624531, 0.541681695426548,
          -1.64473294029318, 1.21644947798893, -4.64495277344251, 3.32682243986647},
         {-3.12418484816708, 1.67917510264378, 4.20889148253205, 0.0238872151232209, 1.84517971790837,
          -0.0472865464398709, 0.45417864715579, 2.77326133469285, -0.0864127147052114, -0.043090496793568,
          0.0818085213313829, -3.5833831117847, -20.486485474037, 1.65409179936228, -16.9036825781639},
         {1.29576735058432, -1.35603456743421, -1.15769439804082, -0.0154091699115222, -0.269894747953877,
          -0.0394746698114206, 0.335044065088222, -0.22596289454307, 0.0569226067833895, 0.0139424302294552,
          0.227517248979103, 0.515037414733253, -38.6451977653538, -0.369391881638122, -38.865480503087},
         {1.63617381209047, 0.795864036818857, 1.90016066742316, 0.0467212446941904, 0.65176670912247,
          -1.16342358202322, 1.02184443744637, -0.622587887454424, 1.01573495927435, -0.0357394288739631,
          1.04710678902117, 0.523112256626539, -0.682522893704988, 2.55083320566512, -1.40050169118396},
         {4.19293777412234, 4.67799152981776, -11.7906973881412, -0.175080462292134, 0.202943879307152,
          0.073123424260982, 0.171886970775394, -1.99621394284143, -0.620863011448878, -0.701504892619358,
          0.466871240179653, 0.0373157802834512, -4.87382846232702, 0.944098498561362, -7.48318417491154},
         {0.955070191144811, -1.60872852342894, -0.426556565428086, 0.555323969694957, -0.363458953202896,
          -0.715759872650188, -4.31464898405827, -1.28920059456462, 0.468591168381666, 0.100702406074328,
          0.128399906015864, 0.981495493810682, -18.9503790087465, 1.36250556475081, -20.7934900245492},
         {0.483056546142342, 0.492313431835093, -0.9413727511542, -0.24925877210127, 0.299766547130769,
          0.226307489152304, 1.97570813480127, -0.776778111260296, 0.931247394761006, -1.61277412651141,
          2.15646144635877, 1.02972548079514, -1.21652811622795, 0.821144267466446, -1.60454016066408}}};

    // weight matrix from layer 1 to 2
    constexpr auto fWeightMatrix1to2 = std::array<float, 20>{
        {-0.820352922809023, 0.264346065695722,  0.567896967985917, -3.60579398981565, 1.75641720224741,
         -6.75140352471911,  1.08065372997106,   1.03573294487812,  0.480575276656891, 1.10829050829656,
         1.09584094877543,   1.75867412589606,   0.500060756477766, -2.91395932292689, -2.72566447103699,
         -1.20356482951461,  -0.812731893845809, -1.15848200505697, 1.0625317594135,   1.26793430175215}};

    constexpr auto fMin = std::array<float, 14>{{3, 2.7475274833e-10, -1, 9, 0.0102619789541, 2, 0, 0.00715538021177, 0,
                                                 212, 155, 1.11951160431, 7.11249446869, 1.44909417629}};

    constexpr auto fMax = std::array<float, 14>{{21, 102.336967468, 37, 15, 66.5717773438, 9, 9, 105.609642029, 2, 8281,
                                                 5250, 137.525756836, 59177, 7.59725427628}};

    // Normalization transformation
    constexpr auto transformer = TMV::Utils::Transformer{fMin, fMax};

    // the training input variables
    constexpr auto validator = TMV::Utils::Validator{
        "ReadGhostProbabilityLong",
        std::tuple{"UpgradeGhostInfo_obsVP", "UpgradeGhostInfo_FitVeloChi2", "UpgradeGhostInfo_FitVeloNDoF",
                   "UpgradeGhostInfo_obsFT", "UpgradeGhostInfo_FitTChi2", "UpgradeGhostInfo_FitTNDoF",
                   "UpgradeGhostInfo_obsUT", "UpgradeGhostInfo_FitMatchChi2", "UpgradeGhostInfo_UToutlier",
                   "UpgradeGhostInfo_veloHits", "UpgradeGhostInfo_utHits", "TRACK_CHI2", "TRACK_PT", "TRACK_ETA"}};

    constexpr auto l0To1 = TMV::Utils::Layer{fWeightMatrix0to1, ActivationFnc};
    constexpr auto l1To2 = TMV::Utils::Layer{fWeightMatrix1to2, OutputActivationFnc};
    constexpr auto MVA   = TMV::Utils::MVA{validator, transformer, 0, l0To1, l1To2};
  } // namespace
} // namespace Data::ReadGhostProbabilityLong

//_______________________________________________________________________

struct ReadGhostProbabilityLong final {

  // constructor
  ReadGhostProbabilityLong( LHCb::span<const std::string_view, 14> theInputVars ) {
    Data::ReadGhostProbabilityLong::MVA.validate( theInputVars );
  }

  // the classifier response
  // "inputValues" is a vector of input values in the same order as the
  // variables given to the constructor
  static constexpr auto GetMvaValue( LHCb::span<const float, 14> input ) {
    return Data::ReadGhostProbabilityLong::MVA( input );
  }
};
