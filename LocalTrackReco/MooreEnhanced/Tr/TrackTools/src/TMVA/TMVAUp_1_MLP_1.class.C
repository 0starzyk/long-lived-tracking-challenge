/*****************************************************************************\
* (c) Copyright 2000-2018 CERN for the benefit of the LHCb Collaboration      *
*                                                                             *
* This software is distributed under the terms of the GNU General Public      *
* Licence version 3 (GPL Version 3), copied verbatim in the file "COPYING".   *
*                                                                             *
* In applying this licence, CERN does not waive the privileges and immunities *
* granted to it by virtue of its status as an Intergovernmental Organization  *
* or submit itself to any jurisdiction.                                       *
\*****************************************************************************/
// Class: ReadMLP_1
// Automatically generated by MethodBase::MakeClass
//

/* configuration options =====================================================

#GEN -*-*-*-*-*-*-*-*-*-*-*- general info -*-*-*-*-*-*-*-*-*-*-*-

Method         : MLP::MLP_1
TMVA Release   : 4.2.1         [262657]
ROOT Release   : 6.06/00       [394752]
Creator        : hyin
Date           : Thu May 19 21:40:34 2016
Host           : Linux lcgapp-slc6-x86-64-7.cern.ch 2.6.32-573.3.1.el6.x86_64 #1 SMP Fri Aug 14 10:45:09 CEST 2015
x86_64 x86_64 x86_64 GNU/Linux Dir            : /afs/cern.ch/work/h/hyin/workspace/GhostTrack/Upgrade_train/Sigmoid
Training events: 4577931
Analysis type  : [Classification]


#OPT -*-*-*-*-*-*-*-*-*-*-*-*- options -*-*-*-*-*-*-*-*-*-*-*-*-

# Set by User:
NCycles: "600" [Number of training cycles]
HiddenLayers: "N+5" [Specification of hidden layer architecture]
NeuronType: "sigmoid" [Neuron activation function type]
EstimatorType: "CE" [MSE (Mean Square Estimator) for Gaussian Likelihood or CE(Cross-Entropy) for Bernoulli Likelihood]
V: "False" [Verbose output (short form of "VerbosityLevel" below - overrides the latter one)]
VarTransform: "N" [List of variable transformations performed before training, e.g.,
"D_Background,P_Signal,G,N_AllClasses" for: "Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for
the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is
assumed)"] H: "False" [Print method-specific help message] CreateMVAPdfs: "True" [Create PDFs for classifier outputs
(signal and background)] TestRate: "5" [Test for overtraining performed at each #th epochs] UseRegulator: "False" [Use
regulator to avoid over-training] # Default: RandomSeed: "1" [Random seed for initial synapse weights (0 means unique
seed for each run; default value '1')] NeuronInputType: "sum" [Neuron input function type] VerbosityLevel: "Default"
[Verbosity level] IgnoreNegWeightsInTraining: "False" [Events with negative weights are ignored in the training (but are
included for testing and performance evaluation)] TrainingMethod: "BP" [Train with Back-Propagation (BP), BFGS Algorithm
(BFGS), or Genetic Algorithm (GA - slower and worse)] LearningRate: "2.000000e-02" [ANN learning rate parameter]
DecayRate: "1.000000e-02" [Decay rate for learning parameter]
EpochMonitoring: "False" [Provide epoch-wise monitoring plots according to TestRate (caution: causes big ROOT output
file!)] Sampling: "1.000000e+00" [Only 'Sampling' (randomly selected) events are trained each epoch] SamplingEpoch:
"1.000000e+00" [Sampling is used for the first 'SamplingEpoch' epochs, afterwards, all events are taken for training]
SamplingImportance: "1.000000e+00" [ The sampling weights of events in epochs which successful (worse estimator than
before) are multiplied with SamplingImportance, else they are divided.] SamplingTraining: "True" [The training sample is
sampled] SamplingTesting: "False" [The testing sample is sampled] ResetStep: "50" [How often BFGS should reset history]
Tau: "3.000000e+00" [LineSearch "size step"]
BPMode: "sequential" [Back-propagation learning mode: sequential or batch]
BatchSize: "-1" [Batch size: number of events/batch, only set if in Batch Mode, -1 for BatchSize=number_of_events]
ConvergenceImprove: "1.000000e-30" [Minimum improvement which counts as improvement (<0 means automatic convergence
check is turned off)] ConvergenceTests: "-1" [Number of steps (without improvement) required for convergence (<0 means
automatic convergence check is turned off)] UpdateLimit: "10000" [Maximum times of regulator update] CalculateErrors:
"False" [Calculates inverse Hessian matrix at the end of the training to be able to calculate the uncertainties of an
MVA value] WeightRange: "1.000000e+00" [Take the events for the estimator calculations from small deviations from the
desired value to large deviations only over the weight range]
##


#VAR -*-*-*-*-*-*-*-*-*-*-*-* variables *-*-*-*-*-*-*-*-*-*-*-*-

NVar 9
UpgradeGhostInfo_obsVP        UpgradeGhostInfo_obsVP        UpgradeGhostInfo_obsVP        UpgradeGhostInfo_obsVP 'F'
[3,21] UpgradeGhostInfo_expVP        UpgradeGhostInfo_expVP        UpgradeGhostInfo_expVP        UpgradeGhostInfo_expVP
'F'    [2,30] UpgradeGhostInfo_FitVeloChi2  UpgradeGhostInfo_FitVeloChi2  UpgradeGhostInfo_FitVeloChi2
UpgradeGhostInfo_FitVeloChi2                                    'F'    [0,7147.61376953] UpgradeGhostInfo_FitVeloNDoF
UpgradeGhostInfo_FitVeloNDoF  UpgradeGhostInfo_FitVeloNDoF  UpgradeGhostInfo_FitVeloNDoF 'F'    [1,37]
UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits 'F'
[95,9416] UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits UpgradeGhostInfo_utHits
'F'    [113,5736] TRACK_CHI2                    TRACK_CHI2                    TRACK_CHI2                    TRACK_CHI2
'F'    [0,7147.61376953] TRACK_NDOF                    TRACK_NDOF                    TRACK_NDOF TRACK_NDOF 'F'    [1,37]
TRACK_ETA                     TRACK_ETA                     TRACK_ETA                     TRACK_ETA 'F'
[1.30233120918,10.1270008087] NSpec 0


============================================================================ */

#include <array>
#include <cmath>
#include <iostream>
#include <string>
#include <vector>

#ifndef IClassifierReader__def
#  define IClassifierReader__def

class IClassifierReader {

public:
  // constructor
  IClassifierReader() : fStatusIsClean( true ) {}
  virtual ~IClassifierReader() {}

  // return classifier response
  virtual double GetMvaValue( const std::vector<double>& inputValues ) const = 0;

  // returns classifier status
  bool IsStatusClean() const { return fStatusIsClean; }

protected:
  bool fStatusIsClean;
};

#endif

class ReadMLP_1 : public IClassifierReader {

public:
  // constructor
  ReadMLP_1( const std::vector<std::string>& theInputVars )
      : IClassifierReader(), fClassName( "ReadMLP_1" ), fNvars( 9 ), fIsNormalised( false ) {
    // the training input variables
    const char* inputVars[] = {"UpgradeGhostInfo_obsVP",
                               "UpgradeGhostInfo_expVP",
                               "UpgradeGhostInfo_FitVeloChi2",
                               "UpgradeGhostInfo_FitVeloNDoF",
                               "UpgradeGhostInfo_veloHits",
                               "UpgradeGhostInfo_utHits",
                               "TRACK_CHI2",
                               "TRACK_NDOF",
                               "TRACK_ETA"};

    // sanity checks
    if ( theInputVars.size() <= 0 ) {
      std::cout << "Problem in class \"" << fClassName << "\": empty input vector" << std::endl;
      fStatusIsClean = false;
    }

    if ( theInputVars.size() != fNvars ) {
      std::cout << "Problem in class \"" << fClassName
                << "\": mismatch in number of input values: " << theInputVars.size() << " != " << fNvars << std::endl;
      fStatusIsClean = false;
    }

    // validate input variables
    for ( size_t ivar = 0; ivar < theInputVars.size(); ivar++ ) {
      if ( theInputVars[ivar] != inputVars[ivar] ) {
        std::cout << "Problem in class \"" << fClassName << "\": mismatch in input variable names" << std::endl
                  << " for variable [" << ivar << "]: " << theInputVars[ivar].c_str() << " != " << inputVars[ivar]
                  << std::endl;
        fStatusIsClean = false;
      }
    }

    // initialize min and max vectors (for normalisation)
    fVmin[0] = -1;
    fVmax[0] = 1;
    fVmin[1] = -1;
    fVmax[1] = 1;
    fVmin[2] = -1;
    fVmax[2] = 0.99999988079071;
    fVmin[3] = -1;
    fVmax[3] = 1;
    fVmin[4] = -1;
    fVmax[4] = 1;
    fVmin[5] = -1;
    fVmax[5] = 1;
    fVmin[6] = -1;
    fVmax[6] = 0.99999988079071;
    fVmin[7] = -1;
    fVmax[7] = 1;
    fVmin[8] = -1;
    fVmax[8] = 1;

    // initialize input variable types
    fType[0] = 'F';
    fType[1] = 'F';
    fType[2] = 'F';
    fType[3] = 'F';
    fType[4] = 'F';
    fType[5] = 'F';
    fType[6] = 'F';
    fType[7] = 'F';
    fType[8] = 'F';

    // initialize constants
    Initialize();

    // initialize transformation
    InitTransform();
  }

  // the classifier response
  // "inputValues" is a vector of input values in the same order as the
  // variables given to the constructor
  double GetMvaValue( const std::vector<double>& inputValues ) const override;

private:
  // input variable transformation

  double fMin_1[3][9];
  double fMax_1[3][9];
  void   InitTransform_1();
  void   Transform_1( std::vector<double>& iv, int sigOrBgd ) const;
  void   InitTransform();
  void   Transform( std::vector<double>& iv, int sigOrBgd ) const;

  // common member variables
  const char* fClassName;

  const size_t fNvars;
  size_t       GetNvar() const { return fNvars; }
  char         GetType( int ivar ) const { return fType[ivar]; }

  // normalisation of input variables
  const bool fIsNormalised;
  bool       IsNormalised() const { return fIsNormalised; }
  double     fVmin[9];
  double     fVmax[9];
  double     NormVariable( double x, double xmin, double xmax ) const {
    // normalise to output range: [-1, 1]
    return 2 * ( x - xmin ) / ( xmax - xmin ) - 1.0;
  }

  // type of input variable: 'F' or 'I'
  char fType[9];

  // initialize internal variables
  void   Initialize();
  double GetMvaValue__( const std::vector<double>& inputValues ) const;

  // private members (method specific)

  double ActivationFnc( double x ) const;
  double OutputActivationFnc( double x ) const;

  int    fLayers;
  int    fLayerSize[3];
  double fWeightMatrix0to1[15][10]; // weight matrix from layer 0 to 1
  double fWeightMatrix1to2[1][15];  // weight matrix from layer 1 to 2
};

inline void ReadMLP_1::Initialize() {
  // build network structure
  fLayers       = 3;
  fLayerSize[0] = 10;
  fLayerSize[1] = 15;
  fLayerSize[2] = 1;
  // weight matrix from layer 0 to 1
  fWeightMatrix0to1[0][0]  = 16.2828186236537;
  fWeightMatrix0to1[1][0]  = -6.23480463615107;
  fWeightMatrix0to1[2][0]  = -7.42738886486036;
  fWeightMatrix0to1[3][0]  = 8.66055380972631;
  fWeightMatrix0to1[4][0]  = -10.2282408833839;
  fWeightMatrix0to1[5][0]  = 24.3274522034814;
  fWeightMatrix0to1[6][0]  = -8.47042386706255;
  fWeightMatrix0to1[7][0]  = 7.61495281732919;
  fWeightMatrix0to1[8][0]  = -0.484310749667001;
  fWeightMatrix0to1[9][0]  = -23.6169361401487;
  fWeightMatrix0to1[10][0] = 11.503416879301;
  fWeightMatrix0to1[11][0] = -0.788471015616361;
  fWeightMatrix0to1[12][0] = -7.30377796550107;
  fWeightMatrix0to1[13][0] = 1.75261066283903;
  fWeightMatrix0to1[0][1]  = -17.4171104004791;
  fWeightMatrix0to1[1][1]  = -4.03991330911563;
  fWeightMatrix0to1[2][1]  = 16.7058894470955;
  fWeightMatrix0to1[3][1]  = 33.3661314959589;
  fWeightMatrix0to1[4][1]  = 11.8484309778261;
  fWeightMatrix0to1[5][1]  = -2.79529451965477;
  fWeightMatrix0to1[6][1]  = -19.9112970582279;
  fWeightMatrix0to1[7][1]  = -8.74659789553756;
  fWeightMatrix0to1[8][1]  = 6.06213971324003;
  fWeightMatrix0to1[9][1]  = 0.987650006001498;
  fWeightMatrix0to1[10][1] = -7.83021316307761;
  fWeightMatrix0to1[11][1] = 1.34222604241939;
  fWeightMatrix0to1[12][1] = 0.282826792935379;
  fWeightMatrix0to1[13][1] = 1.68860044718044;
  fWeightMatrix0to1[0][2]  = -4.50954233268855;
  fWeightMatrix0to1[1][2]  = -1.01465703077778;
  fWeightMatrix0to1[2][2]  = 8.27736828086451;
  fWeightMatrix0to1[3][2]  = 5.80314543493926;
  fWeightMatrix0to1[4][2]  = -4.10304202828745;
  fWeightMatrix0to1[5][2]  = -16.232881481429;
  fWeightMatrix0to1[6][2]  = -5.74216418357387;
  fWeightMatrix0to1[7][2]  = -4.98208704123318;
  fWeightMatrix0to1[8][2]  = 12.5956861345746;
  fWeightMatrix0to1[9][2]  = -4.45778625361008;
  fWeightMatrix0to1[10][2] = -3.68700630445955;
  fWeightMatrix0to1[11][2] = 2.35208123429568;
  fWeightMatrix0to1[12][2] = -0.221409313870167;
  fWeightMatrix0to1[13][2] = 5.17043168681937;
  fWeightMatrix0to1[0][3]  = 7.39226977931328;
  fWeightMatrix0to1[1][3]  = 0.589904023084744;
  fWeightMatrix0to1[2][3]  = 4.07042427484318;
  fWeightMatrix0to1[3][3]  = -10.6279387844539;
  fWeightMatrix0to1[4][3]  = 2.84594519932427;
  fWeightMatrix0to1[5][3]  = 4.48168114285543;
  fWeightMatrix0to1[6][3]  = 12.6955671918181;
  fWeightMatrix0to1[7][3]  = 7.22526443827389;
  fWeightMatrix0to1[8][3]  = -10.073437872243;
  fWeightMatrix0to1[9][3]  = 18.9184322390703;
  fWeightMatrix0to1[10][3] = -3.8809787378735;
  fWeightMatrix0to1[11][3] = -0.611864962415138;
  fWeightMatrix0to1[12][3] = 2.63756355018874;
  fWeightMatrix0to1[13][3] = -0.925957795342575;
  fWeightMatrix0to1[0][4]  = -0.439209482235534;
  fWeightMatrix0to1[1][4]  = -2.92305891747314;
  fWeightMatrix0to1[2][4]  = 0.0408211386743666;
  fWeightMatrix0to1[3][4]  = -1.67017076276316;
  fWeightMatrix0to1[4][4]  = -2.46703630695555;
  fWeightMatrix0to1[5][4]  = -1.08157027148393;
  fWeightMatrix0to1[6][4]  = 1.16143173612176;
  fWeightMatrix0to1[7][4]  = -0.80753154633224;
  fWeightMatrix0to1[8][4]  = 0.502280322656076;
  fWeightMatrix0to1[9][4]  = -0.77020632835961;
  fWeightMatrix0to1[10][4] = 1.30511750784529;
  fWeightMatrix0to1[11][4] = -5.69547752866875;
  fWeightMatrix0to1[12][4] = 0.264807591939548;
  fWeightMatrix0to1[13][4] = -0.419727557068688;
  fWeightMatrix0to1[0][5]  = 0.0904384919281153;
  fWeightMatrix0to1[1][5]  = 0.356577226904203;
  fWeightMatrix0to1[2][5]  = -0.272966169706222;
  fWeightMatrix0to1[3][5]  = 1.25654078200896;
  fWeightMatrix0to1[4][5]  = 1.39307959931;
  fWeightMatrix0to1[5][5]  = -0.690802884422078;
  fWeightMatrix0to1[6][5]  = -0.0190991100939976;
  fWeightMatrix0to1[7][5]  = 0.769429512700403;
  fWeightMatrix0to1[8][5]  = 0.225621543373322;
  fWeightMatrix0to1[9][5]  = 0.278013524299274;
  fWeightMatrix0to1[10][5] = -0.998949563363163;
  fWeightMatrix0to1[11][5] = 0.507822964141801;
  fWeightMatrix0to1[12][5] = -0.464926053327439;
  fWeightMatrix0to1[13][5] = -0.181850612606329;
  fWeightMatrix0to1[0][6]  = -3.49719202142054;
  fWeightMatrix0to1[1][6]  = -1.3692259943359;
  fWeightMatrix0to1[2][6]  = 8.75149506119758;
  fWeightMatrix0to1[3][6]  = 3.03566239646263;
  fWeightMatrix0to1[4][6]  = -5.35946576787885;
  fWeightMatrix0to1[5][6]  = -17.769032146652;
  fWeightMatrix0to1[6][6]  = -6.18527182968072;
  fWeightMatrix0to1[7][6]  = -4.82792928759723;
  fWeightMatrix0to1[8][6]  = 15.6692759857472;
  fWeightMatrix0to1[9][6]  = -7.82707864310399;
  fWeightMatrix0to1[10][6] = -3.30972253956305;
  fWeightMatrix0to1[11][6] = 2.84933882190411;
  fWeightMatrix0to1[12][6] = -2.27217458630867;
  fWeightMatrix0to1[13][6] = 2.78566375099553;
  fWeightMatrix0to1[0][7]  = 4.03933776857451;
  fWeightMatrix0to1[1][7]  = -0.521928457784779;
  fWeightMatrix0to1[2][7]  = 2.89419537336137;
  fWeightMatrix0to1[3][7]  = -10.6646500041509;
  fWeightMatrix0to1[4][7]  = 2.79199685373012;
  fWeightMatrix0to1[5][7]  = 6.07651628779678;
  fWeightMatrix0to1[6][7]  = 10.0365230518043;
  fWeightMatrix0to1[7][7]  = 7.40697697627837;
  fWeightMatrix0to1[8][7]  = -12.5896778971767;
  fWeightMatrix0to1[9][7]  = 16.7975650398357;
  fWeightMatrix0to1[10][7] = -2.2548629249546;
  fWeightMatrix0to1[11][7] = -1.59272192584562;
  fWeightMatrix0to1[12][7] = 2.69479459086007;
  fWeightMatrix0to1[13][7] = 0.245490409325427;
  fWeightMatrix0to1[0][8]  = -15.3317440470639;
  fWeightMatrix0to1[1][8]  = 1.9337956011842;
  fWeightMatrix0to1[2][8]  = -33.6019277013241;
  fWeightMatrix0to1[3][8]  = -12.9757367010979;
  fWeightMatrix0to1[4][8]  = 4.33487377407609;
  fWeightMatrix0to1[5][8]  = 0.982121701513654;
  fWeightMatrix0to1[6][8]  = 4.88580646832446;
  fWeightMatrix0to1[7][8]  = -6.18244528639951;
  fWeightMatrix0to1[8][8]  = -7.35517477955432;
  fWeightMatrix0to1[9][8]  = 0.588734953872015;
  fWeightMatrix0to1[10][8] = 8.37588775736194;
  fWeightMatrix0to1[11][8] = -2.46252480073921;
  fWeightMatrix0to1[12][8] = -7.61385271953798;
  fWeightMatrix0to1[13][8] = 1.70314106215184;
  fWeightMatrix0to1[0][9]  = -10.2978378070754;
  fWeightMatrix0to1[1][9]  = -10.7852860835686;
  fWeightMatrix0to1[2][9]  = -1.68728634711494;
  fWeightMatrix0to1[3][9]  = 17.8001368461793;
  fWeightMatrix0to1[4][9]  = -6.24401343127293;
  fWeightMatrix0to1[5][9]  = -4.85679797934668;
  fWeightMatrix0to1[6][9]  = -15.7587439473589;
  fWeightMatrix0to1[7][9]  = -6.3487956205156;
  fWeightMatrix0to1[8][9]  = 3.10782233288015;
  fWeightMatrix0to1[9][9]  = -2.78949150324597;
  fWeightMatrix0to1[10][9] = 4.68286110060052;
  fWeightMatrix0to1[11][9] = -3.59720843106363;
  fWeightMatrix0to1[12][9] = -5.27746209703159;
  fWeightMatrix0to1[13][9] = 6.58247649718657;
  // weight matrix from layer 1 to 2
  fWeightMatrix1to2[0][0]  = 4.25185986533643;
  fWeightMatrix1to2[0][1]  = 1.92491188594922;
  fWeightMatrix1to2[0][2]  = -5.28452233481105;
  fWeightMatrix1to2[0][3]  = -1.81407540688526;
  fWeightMatrix1to2[0][4]  = 1.98837586080143;
  fWeightMatrix1to2[0][5]  = 3.90423637644076;
  fWeightMatrix1to2[0][6]  = 6.0118197825525;
  fWeightMatrix1to2[0][7]  = -4.76797264508613;
  fWeightMatrix1to2[0][8]  = -3.68330274597815;
  fWeightMatrix1to2[0][9]  = 3.26393573164837;
  fWeightMatrix1to2[0][10] = -6.03249805056734;
  fWeightMatrix1to2[0][11] = 2.9751738769871;
  fWeightMatrix1to2[0][12] = 6.86174870235402;
  fWeightMatrix1to2[0][13] = 9.15593636560373;
  fWeightMatrix1to2[0][14] = 0.0372045699830416;
}

inline double ReadMLP_1::GetMvaValue__( const std::vector<double>& inputValues ) const {

  std::array<double, 10> fWeights0{{}};
  std::array<double, 15> fWeights1{{}};
  std::array<double, 1>  fWeights2{{}};

  fWeights0.back() = 1.;
  fWeights1.back() = 1.;

  for ( int i = 0; i < fLayerSize[0] - 1; i++ ) fWeights0[i] = inputValues[i];

  // layer 0 to 1
  for ( int o = 0; o < fLayerSize[1] - 1; o++ ) {
    for ( int i = 0; i < fLayerSize[0]; i++ ) {
      double inputVal = fWeightMatrix0to1[o][i] * fWeights0[i];
      fWeights1[o] += inputVal;
    }
    fWeights1[o] = ActivationFnc( fWeights1[o] );
  }
  // layer 1 to 2
  for ( int o = 0; o < fLayerSize[2]; o++ ) {
    for ( int i = 0; i < fLayerSize[1]; i++ ) {
      double inputVal = fWeightMatrix1to2[o][i] * fWeights1[i];
      fWeights2[o] += inputVal;
    }
    fWeights2[o] = OutputActivationFnc( fWeights2[o] );
  }

  return fWeights2[0];
}

double ReadMLP_1::ActivationFnc( double x ) const {
  // sigmoid
  return 1.0 / ( 1.0 + exp( -x ) );
}
double ReadMLP_1::OutputActivationFnc( double x ) const {
  // sigmoid
  return 1.0 / ( 1.0 + exp( -x ) );
}

inline double ReadMLP_1::GetMvaValue( const std::vector<double>& inputValues ) const {
  // classifier response value
  double retval = 0;

  // classifier response, sanity check first
  if ( !IsStatusClean() ) {
    std::cout << "Problem in class \"" << fClassName << "\": cannot return classifier response"
              << " because status is dirty" << std::endl;
    retval = 0;
  } else {
    if ( IsNormalised() ) {
      // normalise variables
      std::vector<double> iV;
      iV.reserve( inputValues.size() );
      int ivar = 0;
      for ( std::vector<double>::const_iterator varIt = inputValues.begin(); varIt != inputValues.end();
            varIt++, ivar++ ) {
        iV.push_back( NormVariable( *varIt, fVmin[ivar], fVmax[ivar] ) );
      }
      Transform( iV, -1 );
      retval = GetMvaValue__( iV );
    } else {
      std::vector<double> iV;
      int                 ivar = 0;
      for ( std::vector<double>::const_iterator varIt = inputValues.begin(); varIt != inputValues.end();
            varIt++, ivar++ ) {
        iV.push_back( *varIt );
      }
      Transform( iV, -1 );
      retval = GetMvaValue__( iV );
    }
  }

  return retval;
}

//_______________________________________________________________________
inline void ReadMLP_1::InitTransform_1() {
  // Normalization transformation, initialisation
  fMin_1[0][0] = 3;
  fMax_1[0][0] = 21;
  fMin_1[1][0] = 3;
  fMax_1[1][0] = 19;
  fMin_1[2][0] = 3;
  fMax_1[2][0] = 21;
  fMin_1[0][1] = 2;
  fMax_1[0][1] = 30;
  fMin_1[1][1] = 2;
  fMax_1[1][1] = 30;
  fMin_1[2][1] = 2;
  fMax_1[2][1] = 30;
  fMin_1[0][2] = 0;
  fMax_1[0][2] = 7147.61376953;
  fMin_1[1][2] = 0;
  fMax_1[1][2] = 1682.71191406;
  fMin_1[2][2] = 0;
  fMax_1[2][2] = 7147.61376953;
  fMin_1[0][3] = 1;
  fMax_1[0][3] = 37;
  fMin_1[1][3] = 1;
  fMax_1[1][3] = 32;
  fMin_1[2][3] = 1;
  fMax_1[2][3] = 37;
  fMin_1[0][4] = 95;
  fMax_1[0][4] = 9416;
  fMin_1[1][4] = 340;
  fMax_1[1][4] = 9416;
  fMin_1[2][4] = 95;
  fMax_1[2][4] = 9416;
  fMin_1[0][5] = 113;
  fMax_1[0][5] = 5736;
  fMin_1[1][5] = 170;
  fMax_1[1][5] = 5736;
  fMin_1[2][5] = 113;
  fMax_1[2][5] = 5736;
  fMin_1[0][6] = 0;
  fMax_1[0][6] = 7147.61376953;
  fMin_1[1][6] = 0;
  fMax_1[1][6] = 1682.71191406;
  fMin_1[2][6] = 0;
  fMax_1[2][6] = 7147.61376953;
  fMin_1[0][7] = 1;
  fMax_1[0][7] = 37;
  fMin_1[1][7] = 1;
  fMax_1[1][7] = 32;
  fMin_1[2][7] = 1;
  fMax_1[2][7] = 37;
  fMin_1[0][8] = 1.30545413494;
  fMax_1[0][8] = 10.1270008087;
  fMin_1[1][8] = 1.30233120918;
  fMax_1[1][8] = 9.71531963348;
  fMin_1[2][8] = 1.30233120918;
  fMax_1[2][8] = 10.1270008087;
}

//_______________________________________________________________________
inline void ReadMLP_1::Transform_1( std::vector<double>& iv, int cls ) const {
  // Normalization transformation
  if ( cls < 0 || cls > 2 ) {
    if ( 2 > 1 )
      cls = 2;
    else
      cls = 2;
  }
  const int nVar = 9;

  std::array<double, nVar> dv;
  for ( int ivar = 0; ivar < nVar; ivar++ ) dv[ivar] = iv[ivar];
  for ( int ivar = 0; ivar < 9; ivar++ ) {
    double offset = fMin_1[cls][ivar];
    double scale  = 1.0 / ( fMax_1[cls][ivar] - fMin_1[cls][ivar] );
    iv[ivar]      = ( dv[ivar] - offset ) * scale * 2 - 1;
  }
}

//_______________________________________________________________________
inline void ReadMLP_1::InitTransform() { InitTransform_1(); }

//_______________________________________________________________________
inline void ReadMLP_1::Transform( std::vector<double>& iv, int sigOrBgd ) const { Transform_1( iv, sigOrBgd ); }
