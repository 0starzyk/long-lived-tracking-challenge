/*****************************************************************************\
* (c) Copyright 2000-2018 CERN for the benefit of the LHCb Collaboration      *
*                                                                             *
* This software is distributed under the terms of the GNU General Public      *
* Licence version 3 (GPL Version 3), copied verbatim in the file "COPYING".   *
*                                                                             *
* In applying this licence, CERN does not waive the privileges and immunities *
* granted to it by virtue of its status as an Intergovernmental Organization  *
* or submit itself to any jurisdiction.                                       *
\*****************************************************************************/
// Class: ReadMLP_4
// Automatically generated by MethodBase::MakeClass
//

/* configuration options =====================================================

#GEN -*-*-*-*-*-*-*-*-*-*-*- general info -*-*-*-*-*-*-*-*-*-*-*-

Method         : MLP::MLP_4
TMVA Release   : 4.2.1         [262657]
ROOT Release   : 6.06/00       [394752]
Creator        : hyin
Date           : Thu May 19 09:33:39 2016
Host           : Linux lcgapp-slc6-x86-64-7.cern.ch 2.6.32-573.3.1.el6.x86_64 #1 SMP Fri Aug 14 10:45:09 CEST 2015
x86_64 x86_64 x86_64 GNU/Linux Dir            : /afs/cern.ch/work/h/hyin/workspace/GhostTrack/Upgrade_train/Sigmoid
Training events: 1330442
Analysis type  : [Classification]


#OPT -*-*-*-*-*-*-*-*-*-*-*-*- options -*-*-*-*-*-*-*-*-*-*-*-*-

# Set by User:
NCycles: "600" [Number of training cycles]
HiddenLayers: "N+5" [Specification of hidden layer architecture]
NeuronType: "sigmoid" [Neuron activation function type]
EstimatorType: "CE" [MSE (Mean Square Estimator) for Gaussian Likelihood or CE(Cross-Entropy) for Bernoulli Likelihood]
V: "False" [Verbose output (short form of "VerbosityLevel" below - overrides the latter one)]
VarTransform: "N" [List of variable transformations performed before training, e.g.,
"D_Background,P_Signal,G,N_AllClasses" for: "Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for
the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is
assumed)"] H: "False" [Print method-specific help message] CreateMVAPdfs: "True" [Create PDFs for classifier outputs
(signal and background)] TestRate: "5" [Test for overtraining performed at each #th epochs] UseRegulator: "False" [Use
regulator to avoid over-training] # Default: RandomSeed: "1" [Random seed for initial synapse weights (0 means unique
seed for each run; default value '1')] NeuronInputType: "sum" [Neuron input function type] VerbosityLevel: "Default"
[Verbosity level] IgnoreNegWeightsInTraining: "False" [Events with negative weights are ignored in the training (but are
included for testing and performance evaluation)] TrainingMethod: "BP" [Train with Back-Propagation (BP), BFGS Algorithm
(BFGS), or Genetic Algorithm (GA - slower and worse)] LearningRate: "2.000000e-02" [ANN learning rate parameter]
DecayRate: "1.000000e-02" [Decay rate for learning parameter]
EpochMonitoring: "False" [Provide epoch-wise monitoring plots according to TestRate (caution: causes big ROOT output
file!)] Sampling: "1.000000e+00" [Only 'Sampling' (randomly selected) events are trained each epoch] SamplingEpoch:
"1.000000e+00" [Sampling is used for the first 'SamplingEpoch' epochs, afterwards, all events are taken for training]
SamplingImportance: "1.000000e+00" [ The sampling weights of events in epochs which successful (worse estimator than
before) are multiplied with SamplingImportance, else they are divided.] SamplingTraining: "True" [The training sample is
sampled] SamplingTesting: "False" [The testing sample is sampled] ResetStep: "50" [How often BFGS should reset history]
Tau: "3.000000e+00" [LineSearch "size step"]
BPMode: "sequential" [Back-propagation learning mode: sequential or batch]
BatchSize: "-1" [Batch size: number of events/batch, only set if in Batch Mode, -1 for BatchSize=number_of_events]
ConvergenceImprove: "1.000000e-30" [Minimum improvement which counts as improvement (<0 means automatic convergence
check is turned off)] ConvergenceTests: "-1" [Number of steps (without improvement) required for convergence (<0 means
automatic convergence check is turned off)] UpdateLimit: "10000" [Maximum times of regulator update] CalculateErrors:
"False" [Calculates inverse Hessian matrix at the end of the training to be able to calculate the uncertainties of an
MVA value] WeightRange: "1.000000e+00" [Take the events for the estimator calculations from small deviations from the
desired value to large deviations only over the weight range]
##


#VAR -*-*-*-*-*-*-*-*-*-*-*-* variables *-*-*-*-*-*-*-*-*-*-*-*-

NVar 13
UpgradeGhostInfo_obsVP        UpgradeGhostInfo_obsVP        UpgradeGhostInfo_obsVP        UpgradeGhostInfo_obsVP 'F'
[3,21] UpgradeGhostInfo_expVP        UpgradeGhostInfo_expVP        UpgradeGhostInfo_expVP        UpgradeGhostInfo_expVP
'F'    [1,28] UpgradeGhostInfo_FitVeloChi2  UpgradeGhostInfo_FitVeloChi2  UpgradeGhostInfo_FitVeloChi2
UpgradeGhostInfo_FitVeloChi2                                    'F'    [4.13141656302e-12,95.8944168091]
UpgradeGhostInfo_FitVeloNDoF  UpgradeGhostInfo_FitVeloNDoF  UpgradeGhostInfo_FitVeloNDoF  UpgradeGhostInfo_FitVeloNDoF
'F'    [-1,37] UpgradeGhostInfo_obsUT        UpgradeGhostInfo_obsUT        UpgradeGhostInfo_obsUT UpgradeGhostInfo_obsUT
'F'    [3,10] UpgradeGhostInfo_expUTHitExpectation UpgradeGhostInfo_expUTHitExpectation
UpgradeGhostInfo_expUTHitExpectation UpgradeGhostInfo_expUTHitExpectation                                          'F'
[2,9] UpgradeGhostInfo_UToutlier    UpgradeGhostInfo_UToutlier    UpgradeGhostInfo_UToutlier UpgradeGhostInfo_UToutlier
'F'    [0,2] UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits
UpgradeGhostInfo_veloHits                                       'F'    [119,9416] UpgradeGhostInfo_utHits
UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits 'F'    [113,5736] TRACK_CHI2
TRACK_CHI2                    TRACK_CHI2                    TRACK_CHI2 'F'    [0.000139865689562,109.426490784]
TRACK_NDOF                    TRACK_NDOF                    TRACK_NDOF                    TRACK_NDOF 'F'    [2,42]
TRACK_PT                      TRACK_PT                      TRACK_PT                      TRACK_PT 'F'
[9.85505962372,9514533] TRACK_ETA                     TRACK_ETA                     TRACK_ETA TRACK_ETA 'F'
[1.57074308395,6.44915056229] NSpec 0


============================================================================ */

#include <array>
#include <cmath>
#include <iostream>
#include <string>
#include <vector>

#ifndef IClassifierReader__def
#  define IClassifierReader__def

class IClassifierReader {

public:
  // constructor
  IClassifierReader() : fStatusIsClean( true ) {}
  virtual ~IClassifierReader() {}

  // return classifier response
  virtual double GetMvaValue( const std::vector<double>& inputValues ) const = 0;

  // returns classifier status
  bool IsStatusClean() const { return fStatusIsClean; }

protected:
  bool fStatusIsClean;
};

#endif

class ReadMLP_4 : public IClassifierReader {

public:
  // constructor
  ReadMLP_4( const std::vector<std::string>& theInputVars )
      : IClassifierReader(), fClassName( "ReadMLP_4" ), fNvars( 13 ), fIsNormalised( false ) {
    // the training input variables
    const char* inputVars[] = {"UpgradeGhostInfo_obsVP",
                               "UpgradeGhostInfo_expVP",
                               "UpgradeGhostInfo_FitVeloChi2",
                               "UpgradeGhostInfo_FitVeloNDoF",
                               "UpgradeGhostInfo_obsUT",
                               "UpgradeGhostInfo_expUTHitExpectation",
                               "UpgradeGhostInfo_UToutlier",
                               "UpgradeGhostInfo_veloHits",
                               "UpgradeGhostInfo_utHits",
                               "TRACK_CHI2",
                               "TRACK_NDOF",
                               "TRACK_PT",
                               "TRACK_ETA"};

    // sanity checks
    if ( theInputVars.size() <= 0 ) {
      std::cout << "Problem in class \"" << fClassName << "\": empty input vector" << std::endl;
      fStatusIsClean = false;
    }

    if ( theInputVars.size() != fNvars ) {
      std::cout << "Problem in class \"" << fClassName
                << "\": mismatch in number of input values: " << theInputVars.size() << " != " << fNvars << std::endl;
      fStatusIsClean = false;
    }

    // validate input variables
    for ( size_t ivar = 0; ivar < theInputVars.size(); ivar++ ) {
      if ( theInputVars[ivar] != inputVars[ivar] ) {
        std::cout << "Problem in class \"" << fClassName << "\": mismatch in input variable names" << std::endl
                  << " for variable [" << ivar << "]: " << theInputVars[ivar].c_str() << " != " << inputVars[ivar]
                  << std::endl;
        fStatusIsClean = false;
      }
    }

    // initialize min and max vectors (for normalisation)
    fVmin[0]  = -1;
    fVmax[0]  = 1;
    fVmin[1]  = -1;
    fVmax[1]  = 1;
    fVmin[2]  = -1;
    fVmax[2]  = 1;
    fVmin[3]  = -1;
    fVmax[3]  = 1;
    fVmin[4]  = -1;
    fVmax[4]  = 1;
    fVmin[5]  = -1;
    fVmax[5]  = 1;
    fVmin[6]  = -1;
    fVmax[6]  = 1;
    fVmin[7]  = -1;
    fVmax[7]  = 1;
    fVmin[8]  = -1;
    fVmax[8]  = 1;
    fVmin[9]  = -1;
    fVmax[9]  = 1;
    fVmin[10] = -1;
    fVmax[10] = 1;
    fVmin[11] = -1;
    fVmax[11] = 1;
    fVmin[12] = -1;
    fVmax[12] = 1;

    // initialize input variable types
    fType[0]  = 'F';
    fType[1]  = 'F';
    fType[2]  = 'F';
    fType[3]  = 'F';
    fType[4]  = 'F';
    fType[5]  = 'F';
    fType[6]  = 'F';
    fType[7]  = 'F';
    fType[8]  = 'F';
    fType[9]  = 'F';
    fType[10] = 'F';
    fType[11] = 'F';
    fType[12] = 'F';

    // initialize constants
    Initialize();

    // initialize transformation
    InitTransform();
  }

  // the classifier response
  // "inputValues" is a vector of input values in the same order as the
  // variables given to the constructor
  double GetMvaValue( const std::vector<double>& inputValues ) const override;

private:
  // input variable transformation

  double fMin_1[3][13];
  double fMax_1[3][13];
  void   InitTransform_1();
  void   Transform_1( std::vector<double>& iv, int sigOrBgd ) const;
  void   InitTransform();
  void   Transform( std::vector<double>& iv, int sigOrBgd ) const;

  // common member variables
  const char* fClassName;

  const size_t fNvars;
  size_t       GetNvar() const { return fNvars; }
  char         GetType( int ivar ) const { return fType[ivar]; }

  // normalisation of input variables
  const bool fIsNormalised;
  bool       IsNormalised() const { return fIsNormalised; }
  double     fVmin[13];
  double     fVmax[13];
  double     NormVariable( double x, double xmin, double xmax ) const {
    // normalise to output range: [-1, 1]
    return 2 * ( x - xmin ) / ( xmax - xmin ) - 1.0;
  }

  // type of input variable: 'F' or 'I'
  char fType[13];

  // initialize internal variables
  void   Initialize();
  double GetMvaValue__( const std::vector<double>& inputValues ) const;

  // private members (method specific)

  double ActivationFnc( double x ) const;
  double OutputActivationFnc( double x ) const;

  int    fLayers;
  int    fLayerSize[3];
  double fWeightMatrix0to1[19][14]; // weight matrix from layer 0 to 1
  double fWeightMatrix1to2[1][19];  // weight matrix from layer 1 to 2
};

inline void ReadMLP_4::Initialize() {
  // build network structure
  fLayers       = 3;
  fLayerSize[0] = 14;
  fLayerSize[1] = 19;
  fLayerSize[2] = 1;
  // weight matrix from layer 0 to 1
  fWeightMatrix0to1[0][0]   = 11.4597381340799;
  fWeightMatrix0to1[1][0]   = 3.30054832960056;
  fWeightMatrix0to1[2][0]   = -7.41771515941301;
  fWeightMatrix0to1[3][0]   = 9.87255993453928;
  fWeightMatrix0to1[4][0]   = -2.77265931394676;
  fWeightMatrix0to1[5][0]   = 19.7757198425185;
  fWeightMatrix0to1[6][0]   = 7.23883169791907;
  fWeightMatrix0to1[7][0]   = -0.0488309817464893;
  fWeightMatrix0to1[8][0]   = -1.04758035594735;
  fWeightMatrix0to1[9][0]   = -3.80591472763654;
  fWeightMatrix0to1[10][0]  = 2.73453389244468;
  fWeightMatrix0to1[11][0]  = -2.80501309873525;
  fWeightMatrix0to1[12][0]  = -10.1525660291429;
  fWeightMatrix0to1[13][0]  = 3.38800316329625;
  fWeightMatrix0to1[14][0]  = 6.48761540544229;
  fWeightMatrix0to1[15][0]  = -1.29941408476667;
  fWeightMatrix0to1[16][0]  = -4.28228418097428;
  fWeightMatrix0to1[17][0]  = -2.36638974023802;
  fWeightMatrix0to1[0][1]   = 1.31330442078132;
  fWeightMatrix0to1[1][1]   = -2.44577310584666;
  fWeightMatrix0to1[2][1]   = -11.5749154430017;
  fWeightMatrix0to1[3][1]   = 0.409701023274845;
  fWeightMatrix0to1[4][1]   = 2.86154364928458;
  fWeightMatrix0to1[5][1]   = 1.54764718442494;
  fWeightMatrix0to1[6][1]   = -9.2556474857739;
  fWeightMatrix0to1[7][1]   = 3.22836620263839;
  fWeightMatrix0to1[8][1]   = -3.06199024011143;
  fWeightMatrix0to1[9][1]   = -11.5294096769579;
  fWeightMatrix0to1[10][1]  = -2.01143072846887;
  fWeightMatrix0to1[11][1]  = 4.14205126322496;
  fWeightMatrix0to1[12][1]  = 13.902079624966;
  fWeightMatrix0to1[13][1]  = -4.04126972628671;
  fWeightMatrix0to1[14][1]  = 4.1787579702707;
  fWeightMatrix0to1[15][1]  = -16.032968478417;
  fWeightMatrix0to1[16][1]  = -7.0913997879937;
  fWeightMatrix0to1[17][1]  = 0.825259763141966;
  fWeightMatrix0to1[0][2]   = 13.639655853333;
  fWeightMatrix0to1[1][2]   = 4.99926291481691;
  fWeightMatrix0to1[2][2]   = 4.13948266063178;
  fWeightMatrix0to1[3][2]   = 11.3542721696037;
  fWeightMatrix0to1[4][2]   = 0.233942444484177;
  fWeightMatrix0to1[5][2]   = -12.2773262197473;
  fWeightMatrix0to1[6][2]   = 6.63741823151362;
  fWeightMatrix0to1[7][2]   = 3.91030583118733;
  fWeightMatrix0to1[8][2]   = -1.89307868044496;
  fWeightMatrix0to1[9][2]   = 12.0899951398624;
  fWeightMatrix0to1[10][2]  = -4.80947686410714;
  fWeightMatrix0to1[11][2]  = -2.25062190693114;
  fWeightMatrix0to1[12][2]  = -0.391031065073871;
  fWeightMatrix0to1[13][2]  = 8.56434116056261;
  fWeightMatrix0to1[14][2]  = -3.89007835257868;
  fWeightMatrix0to1[15][2]  = 5.47574936460372;
  fWeightMatrix0to1[16][2]  = -7.76199403739572;
  fWeightMatrix0to1[17][2]  = 1.0129244394239;
  fWeightMatrix0to1[0][3]   = -10.2902109792522;
  fWeightMatrix0to1[1][3]   = -3.71068862747614;
  fWeightMatrix0to1[2][3]   = 7.13581253081364;
  fWeightMatrix0to1[3][3]   = -15.6982675192141;
  fWeightMatrix0to1[4][3]   = 2.78951423733661;
  fWeightMatrix0to1[5][3]   = -10.1565767748538;
  fWeightMatrix0to1[6][3]   = -8.5002205710039;
  fWeightMatrix0to1[7][3]   = -2.36866917412612;
  fWeightMatrix0to1[8][3]   = 1.29955294878507;
  fWeightMatrix0to1[9][3]   = 5.72878371545404;
  fWeightMatrix0to1[10][3]  = 1.28917027552314;
  fWeightMatrix0to1[11][3]  = -0.5872285037298;
  fWeightMatrix0to1[12][3]  = -1.29668609977721;
  fWeightMatrix0to1[13][3]  = 0.209419627441577;
  fWeightMatrix0to1[14][3]  = -1.722044421808;
  fWeightMatrix0to1[15][3]  = 4.78936786123573;
  fWeightMatrix0to1[16][3]  = 10.6986848599163;
  fWeightMatrix0to1[17][3]  = 0.143702172775555;
  fWeightMatrix0to1[0][4]   = 3.70157392561672;
  fWeightMatrix0to1[1][4]   = -4.55628920166572;
  fWeightMatrix0to1[2][4]   = -0.402944804653759;
  fWeightMatrix0to1[3][4]   = 2.59134803118141;
  fWeightMatrix0to1[4][4]   = -10.4051040302592;
  fWeightMatrix0to1[5][4]   = -6.37878330496697;
  fWeightMatrix0to1[6][4]   = 1.43817546713489;
  fWeightMatrix0to1[7][4]   = 5.77158509455624;
  fWeightMatrix0to1[8][4]   = 1.35604521742577;
  fWeightMatrix0to1[9][4]   = 3.98764605824958;
  fWeightMatrix0to1[10][4]  = -10.2532458243392;
  fWeightMatrix0to1[11][4]  = 11.4683974305763;
  fWeightMatrix0to1[12][4]  = 0.411819596619495;
  fWeightMatrix0to1[13][4]  = 1.22483175516566;
  fWeightMatrix0to1[14][4]  = -22.8732945515823;
  fWeightMatrix0to1[15][4]  = 3.93440713347377;
  fWeightMatrix0to1[16][4]  = -0.936435513028269;
  fWeightMatrix0to1[17][4]  = -2.45287329281592;
  fWeightMatrix0to1[0][5]   = -0.873406065286831;
  fWeightMatrix0to1[1][5]   = -1.75861175473111;
  fWeightMatrix0to1[2][5]   = -0.702568562277289;
  fWeightMatrix0to1[3][5]   = 0.0263513714278072;
  fWeightMatrix0to1[4][5]   = 0.557170428909031;
  fWeightMatrix0to1[5][5]   = 3.72419711622277;
  fWeightMatrix0to1[6][5]   = -0.712853258410712;
  fWeightMatrix0to1[7][5]   = -3.72540083496184;
  fWeightMatrix0to1[8][5]   = -3.35125363956696;
  fWeightMatrix0to1[9][5]   = -0.783904556523037;
  fWeightMatrix0to1[10][5]  = -3.36642569853115;
  fWeightMatrix0to1[11][5]  = 2.94012239385884;
  fWeightMatrix0to1[12][5]  = -0.425569148035528;
  fWeightMatrix0to1[13][5]  = 1.29330319425862;
  fWeightMatrix0to1[14][5]  = 2.59164853085704;
  fWeightMatrix0to1[15][5]  = 1.945518578082;
  fWeightMatrix0to1[16][5]  = 1.00680027058461;
  fWeightMatrix0to1[17][5]  = 1.5599384165437;
  fWeightMatrix0to1[0][6]   = -0.899368175932042;
  fWeightMatrix0to1[1][6]   = 2.33512164865204;
  fWeightMatrix0to1[2][6]   = -0.623462146320748;
  fWeightMatrix0to1[3][6]   = -0.58036856413088;
  fWeightMatrix0to1[4][6]   = 1.32332634362725;
  fWeightMatrix0to1[5][6]   = 1.60905508521112;
  fWeightMatrix0to1[6][6]   = 0.0441477873744712;
  fWeightMatrix0to1[7][6]   = 3.89121638754229;
  fWeightMatrix0to1[8][6]   = 0.280415333361378;
  fWeightMatrix0to1[9][6]   = -8.28452373779972;
  fWeightMatrix0to1[10][6]  = -3.79788909548718;
  fWeightMatrix0to1[11][6]  = -1.75426347957487;
  fWeightMatrix0to1[12][6]  = -0.722692946820957;
  fWeightMatrix0to1[13][6]  = -1.38485987252243;
  fWeightMatrix0to1[14][6]  = 6.50771867429233;
  fWeightMatrix0to1[15][6]  = -1.73689523108443;
  fWeightMatrix0to1[16][6]  = 0.156322412317378;
  fWeightMatrix0to1[17][6]  = -0.292695399998095;
  fWeightMatrix0to1[0][7]   = -0.0508081964989128;
  fWeightMatrix0to1[1][7]   = -1.08518794134144;
  fWeightMatrix0to1[2][7]   = 0.760182553185146;
  fWeightMatrix0to1[3][7]   = 0.918605866919725;
  fWeightMatrix0to1[4][7]   = 0.0939452879621605;
  fWeightMatrix0to1[5][7]   = 0.544670216628336;
  fWeightMatrix0to1[6][7]   = 0.35742938969137;
  fWeightMatrix0to1[7][7]   = -0.201763680587585;
  fWeightMatrix0to1[8][7]   = 0.173723700229126;
  fWeightMatrix0to1[9][7]   = -0.375576198595992;
  fWeightMatrix0to1[10][7]  = -0.0122631907890363;
  fWeightMatrix0to1[11][7]  = 1.04614811828768;
  fWeightMatrix0to1[12][7]  = -0.695147443250536;
  fWeightMatrix0to1[13][7]  = 0.153484526546869;
  fWeightMatrix0to1[14][7]  = 0.865432777207749;
  fWeightMatrix0to1[15][7]  = 0.796407843359395;
  fWeightMatrix0to1[16][7]  = 0.402317534171272;
  fWeightMatrix0to1[17][7]  = -0.369596029890372;
  fWeightMatrix0to1[0][8]   = -0.237860676394762;
  fWeightMatrix0to1[1][8]   = -0.00752673550928151;
  fWeightMatrix0to1[2][8]   = -0.322394956766661;
  fWeightMatrix0to1[3][8]   = -0.336163417178143;
  fWeightMatrix0to1[4][8]   = 0.748411524340436;
  fWeightMatrix0to1[5][8]   = -0.573981248361093;
  fWeightMatrix0to1[6][8]   = -0.0471690291721234;
  fWeightMatrix0to1[7][8]   = 0.341984086317162;
  fWeightMatrix0to1[8][8]   = -0.15594787004931;
  fWeightMatrix0to1[9][8]   = -0.591898205757761;
  fWeightMatrix0to1[10][8]  = 0.261869486221319;
  fWeightMatrix0to1[11][8]  = 0.328011054108588;
  fWeightMatrix0to1[12][8]  = 0.298695068079704;
  fWeightMatrix0to1[13][8]  = -0.235498273733469;
  fWeightMatrix0to1[14][8]  = 0.408398831887074;
  fWeightMatrix0to1[15][8]  = -1.00761095370284;
  fWeightMatrix0to1[16][8]  = -0.412044755326663;
  fWeightMatrix0to1[17][8]  = -3.00541440542797;
  fWeightMatrix0to1[0][9]   = 2.31782017910643;
  fWeightMatrix0to1[1][9]   = 4.02036016364111;
  fWeightMatrix0to1[2][9]   = -5.66810981443457;
  fWeightMatrix0to1[3][9]   = 2.91271105200886;
  fWeightMatrix0to1[4][9]   = 0.153514286516437;
  fWeightMatrix0to1[5][9]   = 12.9909903646143;
  fWeightMatrix0to1[6][9]   = 4.63621002181258;
  fWeightMatrix0to1[7][9]   = -3.57313022632304;
  fWeightMatrix0to1[8][9]   = 1.95749096297874;
  fWeightMatrix0to1[9][9]   = -4.82310034441312;
  fWeightMatrix0to1[10][9]  = 3.83545402692096;
  fWeightMatrix0to1[11][9]  = 1.91212313922902;
  fWeightMatrix0to1[12][9]  = 3.41367636300023;
  fWeightMatrix0to1[13][9]  = -12.0894056753081;
  fWeightMatrix0to1[14][9]  = 4.1626309814281;
  fWeightMatrix0to1[15][9]  = -6.50568377435043;
  fWeightMatrix0to1[16][9]  = -3.55188308894636;
  fWeightMatrix0to1[17][9]  = -1.49376264743232;
  fWeightMatrix0to1[0][10]  = -8.44694026542683;
  fWeightMatrix0to1[1][10]  = -2.66635228403332;
  fWeightMatrix0to1[2][10]  = 9.06305016107459;
  fWeightMatrix0to1[3][10]  = -14.4867348416452;
  fWeightMatrix0to1[4][10]  = -0.772079736446645;
  fWeightMatrix0to1[5][10]  = -11.8358918845559;
  fWeightMatrix0to1[6][10]  = -6.37238315981821;
  fWeightMatrix0to1[7][10]  = -0.0908254083644798;
  fWeightMatrix0to1[8][10]  = 0.480462190833357;
  fWeightMatrix0to1[9][10]  = 5.15291195831095;
  fWeightMatrix0to1[10][10] = -4.26599024127252;
  fWeightMatrix0to1[11][10] = 3.31715293903299;
  fWeightMatrix0to1[12][10] = 0.280761738157976;
  fWeightMatrix0to1[13][10] = 2.71616512904075;
  fWeightMatrix0to1[14][10] = -7.18891355684125;
  fWeightMatrix0to1[15][10] = 5.38846498499447;
  fWeightMatrix0to1[16][10] = 8.96111137665958;
  fWeightMatrix0to1[17][10] = 2.37697515606743;
  fWeightMatrix0to1[0][11]  = -5.26574141149781;
  fWeightMatrix0to1[1][11]  = 0.0508209470478087;
  fWeightMatrix0to1[2][11]  = 5.23420019291823;
  fWeightMatrix0to1[3][11]  = 1.71438384492235;
  fWeightMatrix0to1[4][11]  = 3.44010109182954;
  fWeightMatrix0to1[5][11]  = -1.64671817441298;
  fWeightMatrix0to1[6][11]  = 1.77599677800888;
  fWeightMatrix0to1[7][11]  = 1.51155293619782;
  fWeightMatrix0to1[8][11]  = 7.00293749086362;
  fWeightMatrix0to1[9][11]  = 0.889350095537888;
  fWeightMatrix0to1[10][11] = 13.3181272612993;
  fWeightMatrix0to1[11][11] = -3.4276785549398;
  fWeightMatrix0to1[12][11] = 1.68425122886771;
  fWeightMatrix0to1[13][11] = 0.809036533556468;
  fWeightMatrix0to1[14][11] = 4.94734678137607;
  fWeightMatrix0to1[15][11] = 1.64020377940829;
  fWeightMatrix0to1[16][11] = 3.5967138008673;
  fWeightMatrix0to1[17][11] = 2.36518621818745;
  fWeightMatrix0to1[0][12]  = -3.17914688476969;
  fWeightMatrix0to1[1][12]  = 1.59873337122423;
  fWeightMatrix0to1[2][12]  = 19.8617475617179;
  fWeightMatrix0to1[3][12]  = -1.12619758144995;
  fWeightMatrix0to1[4][12]  = 1.83285105973627;
  fWeightMatrix0to1[5][12]  = -0.742675029739127;
  fWeightMatrix0to1[6][12]  = -3.56129456411856;
  fWeightMatrix0to1[7][12]  = -1.64683443201188;
  fWeightMatrix0to1[8][12]  = 36.4145451642336;
  fWeightMatrix0to1[9][12]  = -5.55370381420651;
  fWeightMatrix0to1[10][12] = 23.1522113150111;
  fWeightMatrix0to1[11][12] = 1.22634700953039;
  fWeightMatrix0to1[12][12] = -7.81866923565101;
  fWeightMatrix0to1[13][12] = -2.47495734531697;
  fWeightMatrix0to1[14][12] = 8.92561000326465;
  fWeightMatrix0to1[15][12] = -3.5103829676242;
  fWeightMatrix0to1[16][12] = -0.897763164698117;
  fWeightMatrix0to1[17][12] = 0.703928833607967;
  fWeightMatrix0to1[0][13]  = 9.12792578393809;
  fWeightMatrix0to1[1][13]  = -2.54762503085409;
  fWeightMatrix0to1[2][13]  = -4.56295558928633;
  fWeightMatrix0to1[3][13]  = -0.744748874818509;
  fWeightMatrix0to1[4][13]  = -4.83355533077772;
  fWeightMatrix0to1[5][13]  = -2.81128496772127;
  fWeightMatrix0to1[6][13]  = -3.7968419690863;
  fWeightMatrix0to1[7][13]  = -1.40464780772966;
  fWeightMatrix0to1[8][13]  = -9.7007610510031;
  fWeightMatrix0to1[9][13]  = -3.0126905085927;
  fWeightMatrix0to1[10][13] = -14.69522135129;
  fWeightMatrix0to1[11][13] = 4.12183964731096;
  fWeightMatrix0to1[12][13] = -3.11155627035317;
  fWeightMatrix0to1[13][13] = -2.21084764901871;
  fWeightMatrix0to1[14][13] = -6.16871302601893;
  fWeightMatrix0to1[15][13] = -3.86033331238431;
  fWeightMatrix0to1[16][13] = -0.645649564549161;
  fWeightMatrix0to1[17][13] = -2.31399992942415;
  // weight matrix from layer 1 to 2
  fWeightMatrix1to2[0][0]  = 2.38841451861398;
  fWeightMatrix1to2[0][1]  = 1.73991173233621;
  fWeightMatrix1to2[0][2]  = 4.05400047939101;
  fWeightMatrix1to2[0][3]  = -3.82388086214705;
  fWeightMatrix1to2[0][4]  = -3.3563636444431;
  fWeightMatrix1to2[0][5]  = -1.89998003573734;
  fWeightMatrix1to2[0][6]  = 3.62887604108725;
  fWeightMatrix1to2[0][7]  = -6.26570775591854;
  fWeightMatrix1to2[0][8]  = -8.28279230542068;
  fWeightMatrix1to2[0][9]  = 0.954373726521419;
  fWeightMatrix1to2[0][10] = 9.57666288586306;
  fWeightMatrix1to2[0][11] = -1.16558117338806;
  fWeightMatrix1to2[0][12] = 2.51740255347014;
  fWeightMatrix1to2[0][13] = 1.84988771242788;
  fWeightMatrix1to2[0][14] = -1.25635982002858;
  fWeightMatrix1to2[0][15] = 1.04146555394861;
  fWeightMatrix1to2[0][16] = 2.17603610420119;
  fWeightMatrix1to2[0][17] = 2.47487201371165;
  fWeightMatrix1to2[0][18] = -2.10704295686975;
}

inline double ReadMLP_4::GetMvaValue__( const std::vector<double>& inputValues ) const {

  std::array<double, 14> fWeights0{{}};
  std::array<double, 19> fWeights1{{}};
  std::array<double, 1>  fWeights2{{}};

  fWeights0.back() = 1.;
  fWeights1.back() = 1.;

  for ( int i = 0; i < fLayerSize[0] - 1; i++ ) fWeights0[i] = inputValues[i];

  // layer 0 to 1
  for ( int o = 0; o < fLayerSize[1] - 1; o++ ) {
    for ( int i = 0; i < fLayerSize[0]; i++ ) {
      double inputVal = fWeightMatrix0to1[o][i] * fWeights0[i];
      fWeights1[o] += inputVal;
    }
    fWeights1[o] = ActivationFnc( fWeights1[o] );
  }
  // layer 1 to 2
  for ( int o = 0; o < fLayerSize[2]; o++ ) {
    for ( int i = 0; i < fLayerSize[1]; i++ ) {
      double inputVal = fWeightMatrix1to2[o][i] * fWeights1[i];
      fWeights2[o] += inputVal;
    }
    fWeights2[o] = OutputActivationFnc( fWeights2[o] );
  }

  return fWeights2[0];
}

double ReadMLP_4::ActivationFnc( double x ) const {
  // sigmoid
  return 1.0 / ( 1.0 + exp( -x ) );
}
double ReadMLP_4::OutputActivationFnc( double x ) const {
  // sigmoid
  return 1.0 / ( 1.0 + exp( -x ) );
}

inline double ReadMLP_4::GetMvaValue( const std::vector<double>& inputValues ) const {
  // classifier response value
  double retval = 0;

  // classifier response, sanity check first
  if ( !IsStatusClean() ) {
    std::cout << "Problem in class \"" << fClassName << "\": cannot return classifier response"
              << " because status is dirty" << std::endl;
    retval = 0;
  } else {
    if ( IsNormalised() ) {
      // normalise variables
      std::vector<double> iV;
      iV.reserve( inputValues.size() );
      int ivar = 0;
      for ( std::vector<double>::const_iterator varIt = inputValues.begin(); varIt != inputValues.end();
            varIt++, ivar++ ) {
        iV.push_back( NormVariable( *varIt, fVmin[ivar], fVmax[ivar] ) );
      }
      Transform( iV, -1 );
      retval = GetMvaValue__( iV );
    } else {
      std::vector<double> iV;
      int                 ivar = 0;
      for ( std::vector<double>::const_iterator varIt = inputValues.begin(); varIt != inputValues.end();
            varIt++, ivar++ ) {
        iV.push_back( *varIt );
      }
      Transform( iV, -1 );
      retval = GetMvaValue__( iV );
    }
  }

  return retval;
}

//_______________________________________________________________________
inline void ReadMLP_4::InitTransform_1() {
  // Normalization transformation, initialisation
  fMin_1[0][0]  = 3;
  fMax_1[0][0]  = 21;
  fMin_1[1][0]  = 3;
  fMax_1[1][0]  = 21;
  fMin_1[2][0]  = 3;
  fMax_1[2][0]  = 21;
  fMin_1[0][1]  = 2;
  fMax_1[0][1]  = 22;
  fMin_1[1][1]  = 1;
  fMax_1[1][1]  = 28;
  fMin_1[2][1]  = 1;
  fMax_1[2][1]  = 28;
  fMin_1[0][2]  = 2.54306620207e-09;
  fMax_1[0][2]  = 95.8944168091;
  fMin_1[1][2]  = 4.13141656302e-12;
  fMax_1[1][2]  = 89.6014328003;
  fMin_1[2][2]  = 4.13141656302e-12;
  fMax_1[2][2]  = 95.8944168091;
  fMin_1[0][3]  = -1;
  fMax_1[0][3]  = 37;
  fMin_1[1][3]  = -1;
  fMax_1[1][3]  = 37;
  fMin_1[2][3]  = -1;
  fMax_1[2][3]  = 37;
  fMin_1[0][4]  = 3;
  fMax_1[0][4]  = 9;
  fMin_1[1][4]  = 3;
  fMax_1[1][4]  = 10;
  fMin_1[2][4]  = 3;
  fMax_1[2][4]  = 10;
  fMin_1[0][5]  = 2;
  fMax_1[0][5]  = 9;
  fMin_1[1][5]  = 2;
  fMax_1[1][5]  = 9;
  fMin_1[2][5]  = 2;
  fMax_1[2][5]  = 9;
  fMin_1[0][6]  = 0;
  fMax_1[0][6]  = 2;
  fMin_1[1][6]  = 0;
  fMax_1[1][6]  = 2;
  fMin_1[2][6]  = 0;
  fMax_1[2][6]  = 2;
  fMin_1[0][7]  = 119;
  fMax_1[0][7]  = 9416;
  fMin_1[1][7]  = 144;
  fMax_1[1][7]  = 9416;
  fMin_1[2][7]  = 119;
  fMax_1[2][7]  = 9416;
  fMin_1[0][8]  = 113;
  fMax_1[0][8]  = 5736;
  fMin_1[1][8]  = 121;
  fMax_1[1][8]  = 5736;
  fMin_1[2][8]  = 113;
  fMax_1[2][8]  = 5736;
  fMin_1[0][9]  = 0.00213921768591;
  fMax_1[0][9]  = 103.377151489;
  fMin_1[1][9]  = 0.000139865689562;
  fMax_1[1][9]  = 109.426490784;
  fMin_1[2][9]  = 0.000139865689562;
  fMax_1[2][9]  = 109.426490784;
  fMin_1[0][10] = 2;
  fMax_1[0][10] = 42;
  fMin_1[1][10] = 2;
  fMax_1[1][10] = 41;
  fMin_1[2][10] = 2;
  fMax_1[2][10] = 42;
  fMin_1[0][11] = 16.0771999359;
  fMax_1[0][11] = 9514533;
  fMin_1[1][11] = 9.85505962372;
  fMax_1[1][11] = 7184650;
  fMin_1[2][11] = 9.85505962372;
  fMax_1[2][11] = 9514533;
  fMin_1[0][12] = 1.57074308395;
  fMax_1[0][12] = 5.68096065521;
  fMin_1[1][12] = 1.58368432522;
  fMax_1[1][12] = 6.44915056229;
  fMin_1[2][12] = 1.57074308395;
  fMax_1[2][12] = 6.44915056229;
}

//_______________________________________________________________________
inline void ReadMLP_4::Transform_1( std::vector<double>& iv, int cls ) const {
  // Normalization transformation
  if ( cls < 0 || cls > 2 ) {
    if ( 2 > 1 )
      cls = 2;
    else
      cls = 2;
  }
  const int nVar = 13;

  std::array<double, nVar> dv;
  for ( int ivar = 0; ivar < nVar; ivar++ ) dv[ivar] = iv[ivar];
  for ( int ivar = 0; ivar < 13; ivar++ ) {
    double offset = fMin_1[cls][ivar];
    double scale  = 1.0 / ( fMax_1[cls][ivar] - fMin_1[cls][ivar] );
    iv[ivar]      = ( dv[ivar] - offset ) * scale * 2 - 1;
  }
}

//_______________________________________________________________________
inline void ReadMLP_4::InitTransform() { InitTransform_1(); }

//_______________________________________________________________________
inline void ReadMLP_4::Transform( std::vector<double>& iv, int sigOrBgd ) const { Transform_1( iv, sigOrBgd ); }
