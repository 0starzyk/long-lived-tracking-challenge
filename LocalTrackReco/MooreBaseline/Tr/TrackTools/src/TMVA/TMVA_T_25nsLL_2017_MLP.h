/*****************************************************************************\
* (c) Copyright 2019 CERN for the benefit of the LHCb Collaboration           *
*                                                                             *
* This software is distributed under the terms of the GNU General Public      *
* Licence version 3 (GPL Version 3), copied verbatim in the file "COPYING".   *
*                                                                             *
* In applying this licence, CERN does not waive the privileges and immunities *
* granted to it by virtue of its status as an Intergovernmental Organization  *
* or submit itself to any jurisdiction.                                       *
\*****************************************************************************/
// Class: ReadMLP_6
// Automatically generated by MethodBase::MakeClass
//

/* configuration options =====================================================

#GEN -*-*-*-*-*-*-*-*-*-*-*- general info -*-*-*-*-*-*-*-*-*-*-*-

Method         : MLP::MLP
TMVA Release   : 4.2.1         [262657]
ROOT Release   : 6.06/08       [394760]
Creator        : mexu
Date           : Thu Jan 31 10:29:47 2019
Host           : Linux lcgapp-slc6-x86-64-2.cern.ch 2.6.32-504.1.3.el6.x86_64 #1 SMP Wed Nov 12 06:58:35 CET 2014 x86_64
x86_64 x86_64 GNU/Linux Dir            : /afs/cern.ch/work/m/mexu/workspace/Ghost_Study/Final_Verstion Training events:
154165 Analysis type  : [Classification]


#OPT -*-*-*-*-*-*-*-*-*-*-*-*- options -*-*-*-*-*-*-*-*-*-*-*-*-

# Set by User:
NCycles: "600" [Number of training cycles]
HiddenLayers: "N+5" [Specification of hidden layer architecture]
NeuronType: "ReLU" [Neuron activation function type]
EstimatorType: "CE" [MSE (Mean Square Estimator) for Gaussian Likelihood or CE(Cross-Entropy) for Bernoulli Likelihood]
V: "False" [Verbose output (short form of "VerbosityLevel" below - overrides the latter one)]
VarTransform: "N" [List of variable transformations performed before training, e.g.,
"D_Background,P_Signal,G,N_AllClasses" for: "Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for
the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is
assumed)"] H: "False" [Print method-specific help message] CreateMVAPdfs: "True" [Create PDFs for classifier outputs
(signal and background)] TestRate: "5" [Test for overtraining performed at each #th epochs] UseRegulator: "False" [Use
regulator to avoid over-training] # Default: RandomSeed: "1" [Random seed for initial synapse weights (0 means unique
seed for each run; default value '1')] NeuronInputType: "sum" [Neuron input function type] VerbosityLevel: "Default"
[Verbosity level] IgnoreNegWeightsInTraining: "False" [Events with negative weights are ignored in the training (but are
included for testing and performance evaluation)] TrainingMethod: "BP" [Train with Back-Propagation (BP), BFGS Algorithm
(BFGS), or Genetic Algorithm (GA - slower and worse)] LearningRate: "2.000000e-02" [ANN learning rate parameter]
DecayRate: "1.000000e-02" [Decay rate for learning parameter]
EpochMonitoring: "False" [Provide epoch-wise monitoring plots according to TestRate (caution: causes big ROOT output
file!)] Sampling: "1.000000e+00" [Only 'Sampling' (randomly selected) events are trained each epoch] SamplingEpoch:
"1.000000e+00" [Sampling is used for the first 'SamplingEpoch' epochs, afterwards, all events are taken for training]
SamplingImportance: "1.000000e+00" [ The sampling weights of events in epochs which successful (worse estimator than
before) are multiplied with SamplingImportance, else they are divided.] SamplingTraining: "True" [The training sample is
sampled] SamplingTesting: "False" [The testing sample is sampled] ResetStep: "50" [How often BFGS should reset history]
Tau: "3.000000e+00" [LineSearch "size step"]
BPMode: "sequential" [Back-propagation learning mode: sequential or batch]
BatchSize: "-1" [Batch size: number of events/batch, only set if in Batch Mode, -1 for BatchSize=number_of_events]
ConvergenceImprove: "1.000000e-30" [Minimum improvement which counts as improvement (<0 means automatic convergence
check is turned off)] ConvergenceTests: "-1" [Number of steps (without improvement) required for convergence (<0 means
automatic convergence check is turned off)] UpdateLimit: "10000" [Maximum times of regulator update] CalculateErrors:
"False" [Calculates inverse Hessian matrix at the end of the training to be able to calculate the uncertainties of an
MVA value] WeightRange: "1.000000e+00" [Take the events for the estimator calculations from small deviations from the
desired value to large deviations only over the weight range]
##


#VAR -*-*-*-*-*-*-*-*-*-*-*-* variables *-*-*-*-*-*-*-*-*-*-*-*-

NVar 9
UpgradeGhostInfo_obsFT        UpgradeGhostInfo_obsFT        UpgradeGhostInfo_obsFT        UpgradeGhostInfo_obsFT 'F'
[9,12] UpgradeGhostInfo_FitTChi2     UpgradeGhostInfo_FitTChi2     UpgradeGhostInfo_FitTChi2 UpgradeGhostInfo_FitTChi2
'F'    [0.00103625666816,1384.39025879] UpgradeGhostInfo_FitTNDoF     UpgradeGhostInfo_FitTNDoF
UpgradeGhostInfo_FitTNDoF     UpgradeGhostInfo_FitTNDoF                                       'F'    [2,7]
UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits 'F'
[212,8281] UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits UpgradeGhostInfo_utHits
'F'    [155,5250] TRACK_CHI2                    TRACK_CHI2                    TRACK_CHI2                    TRACK_CHI2
'F'    [0.000112873793114,1380.37426758] TRACK_NDOF                    TRACK_NDOF                    TRACK_NDOF
TRACK_NDOF                                                      'F'    [2,7] TRACK_PT                      TRACK_PT
TRACK_PT                      TRACK_PT                                                        'F'
[6.42244386673,1426971648] TRACK_ETA                     TRACK_ETA                     TRACK_ETA TRACK_ETA 'F'
[0.671881437302,7.3996925354] NSpec 0


============================================================================ */
#include "Kernel/STLExtensions.h"
#include "Kernel/TMV_utils.h"
#include "vdt/exp.h"
#include <array>
#include <string_view>

namespace Data::ReadGhostProbabilityT {
  namespace {
    constexpr auto ActivationFnc       = []( float x ) { return x > 0 ? x : 0; };
    constexpr auto OutputActivationFnc = []( float x ) {
      // sigmoid
      return 1.f / ( 1.f + vdt::fast_expf( -x ) );
    };

    // build network structure
    // weight matrix from layer 0 to 1
    constexpr auto fWeightMatrix0to1 = std::array<std::array<float, 10>, 14>{
        {{-0.912286815342526, 0.44212138895444, -2.75674824184143, 0.781208852732027, 0.816432996423708,
          0.377642250443819, -1.74452265678322, -0.290114387559118, -0.349928525336334, -1.56546518468252},
         {-0.5201508649408, 2.16064946951325, 0.79720150002215, -2.35130037091707, -3.59902032973346, 0.048739797993407,
          0.44408387386181, 2.00678854381484, -1.80405155695838, -1.74259209923162},
         {-0.000226274525475814, 0.0429948901082986, 0.425861352472204, -1.87689212316632, 2.26191704036192,
          1.20195627401546, 0.901108650839534, -1.61374732666224, -0.0575762563796261, -2.10857823637018},
         {5.42812862604471, 1.43553963655372, 0.866131119529654, 0.316018235137805, 1.88210421463478, 0.836011573392765,
          -1.90121877649199, -0.811982246305338, 0.55998531389186, -0.67436946140515},
         {-1.67445441707893, 0.48052953416344, -0.0989724641768454, 1.1770189326732, -1.48904723407785,
          1.66371750119113, -1.35528763310149, 1.03726590229696, 0.848442219074374, -0.178325120481407},
         {-1.48485281551672, 1.38672463397174, -0.168531369887713, 0.0714385240654162, 0.371160891757707,
          -0.348363566348079, -1.70389390970064, 1.67009473049051, -0.953902005273521, -1.82126649329024},
         {-0.313454931252404, -11.5677543429405, 0.32047175989594, 0.550080626662732, 0.0307150285743112,
          -13.2113611171885, -0.122713451263522, 7.06825222901512, 7.50033869941668, -12.6513148313359},
         {0.621800766267027, -1.53440554987278, -0.360153210205024, 1.13252901063624, -0.0547798711889391,
          -2.65075457601803, -0.205993391428989, 4.67284699128623, 0.533803646818768, -0.276649562765391},
         {-0.139457683020872, 6.44695198863492, -1.77998803483281, -0.0916610683057183, -0.470674049098014,
          6.98080924607284, 1.29284623524213, 0.275001267003561, -18.9058259480569, -1.52020615343035},
         {0.332351527922544, 12.5080930487857, 1.30167782546557, 0.625671846510159, 0.178860276012279, 12.8300310869312,
          -2.07014431843938, -13.0469510165928, 2.49016238832674, 13.7737980362334},
         {0.301595655866369, 15.285522803121, -1.73894925929727, 0.453477755439189, 2.30560003943311, 18.1435689284252,
          -1.35689861934743, -17.2602648331628, 2.33589901422437, 19.5497701257528},
         {-0.43354414962364, -0.242605279544645, 1.03760681244193, 1.44744177952899, -1.01821938259778,
          0.626137707441307, 1.53753055574953, 0.317012736027099, 2.01637516116275, -1.86025079393199},
         {-3.02266757718924, 3.62500468490862, 1.40890006559921, -0.374709528606176, 0.288653907058676,
          3.01410854080481, -0.642976827668046, -4.47856632795789, 1.66538161829695, 3.24386976729901},
         {0.375984847516959, 0.801484433418435, 1.18089934550312, -0.459767080289082, 0.515322146752193,
          1.08141557722614, -1.2050208028398, -0.227514980666386, 6.56293880844706, 2.29873489777121}}};

    constexpr auto fWeightMatrix1to2 = std::array<float, 15>{
        {0.551908761418982, -0.689401811742582, -1.1321330161753, 0.742004705562051, -1.37213407165537,
         1.56809302698732, 1.45468049334574, -2.69653859752506, 1.33671402801066, -2.04410905587656, -1.24400432731689,
         0.775576121632336, -0.768228543802753, -0.696270376649102, 2.04830289906688}};

    constexpr auto fMin =
        std::array<float, 9>{{9, 0.00103625666816, 2, 212, 155, 0.000112873793114, 2, 6.42244386673, 0.671881437302}};

    constexpr auto fMax =
        std::array<float, 9>{{12, 1384.39025879, 7, 8281, 5250, 1380.37426758, 7, 1426971648, 7.3996925354}};

    // Normalization transformation
    constexpr auto transformer = TMV::Utils::Transformer{fMin, fMax};

    // the training input variables
    constexpr auto validator = TMV::Utils::Validator{"ReadGhostProbabilityUpstream",
                                                     std::tuple{"UpgradeGhostInfo_obsFT", "UpgradeGhostInfo_FitTChi2",
                                                                "UpgradeGhostInfo_FitTNDoF",
                                                                "UpgradeGhostInfo_veloHits", "UpgradeGhostInfo_utHits",
                                                                "TRACK_CHI2", "TRACK_NDOF", "TRACK_PT", "TRACK_ETA"}};

    constexpr auto l0To1 = TMV::Utils::Layer{fWeightMatrix0to1, ActivationFnc};
    constexpr auto l1To2 = TMV::Utils::Layer{fWeightMatrix1to2, OutputActivationFnc};
    constexpr auto MVA   = TMV::Utils::MVA{validator, transformer, 0, l0To1, l1To2};
  } // namespace
} // namespace Data::ReadGhostProbabilityT

//_______________________________________________________________________

struct ReadGhostProbabilityT final {

  // constructor
  ReadGhostProbabilityT( LHCb::span<const std::string_view, 9> theInputVars ) {
    Data::ReadGhostProbabilityT::MVA.validate( theInputVars );
  }

  // the classifier response
  // "inputValues" is a vector of input values in the same order as the
  // variables given to the constructor
  static constexpr auto GetMvaValue( LHCb::span<const float, 9> input ) {
    return Data::ReadGhostProbabilityT::MVA( input );
  }
};
