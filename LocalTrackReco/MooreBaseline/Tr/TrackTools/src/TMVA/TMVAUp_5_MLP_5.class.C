/*****************************************************************************\
* (c) Copyright 2000-2018 CERN for the benefit of the LHCb Collaboration      *
*                                                                             *
* This software is distributed under the terms of the GNU General Public      *
* Licence version 3 (GPL Version 3), copied verbatim in the file "COPYING".   *
*                                                                             *
* In applying this licence, CERN does not waive the privileges and immunities *
* granted to it by virtue of its status as an Intergovernmental Organization  *
* or submit itself to any jurisdiction.                                       *
\*****************************************************************************/
// Class: ReadMLP_5
// Automatically generated by MethodBase::MakeClass
//

/* configuration options =====================================================

#GEN -*-*-*-*-*-*-*-*-*-*-*- general info -*-*-*-*-*-*-*-*-*-*-*-

Method         : MLP::MLP_5
TMVA Release   : 4.2.1         [262657]
ROOT Release   : 6.06/00       [394752]
Creator        : hyin
Date           : Thu May 19 07:08:16 2016
Host           : Linux lcgapp-slc6-x86-64-7.cern.ch 2.6.32-573.3.1.el6.x86_64 #1 SMP Fri Aug 14 10:45:09 CEST 2015
x86_64 x86_64 x86_64 GNU/Linux Dir            : /afs/cern.ch/work/h/hyin/workspace/GhostTrack/Upgrade_train/Sigmoid
Training events: 509459
Analysis type  : [Classification]


#OPT -*-*-*-*-*-*-*-*-*-*-*-*- options -*-*-*-*-*-*-*-*-*-*-*-*-

# Set by User:
NCycles: "600" [Number of training cycles]
HiddenLayers: "N+5" [Specification of hidden layer architecture]
NeuronType: "sigmoid" [Neuron activation function type]
EstimatorType: "CE" [MSE (Mean Square Estimator) for Gaussian Likelihood or CE(Cross-Entropy) for Bernoulli Likelihood]
V: "False" [Verbose output (short form of "VerbosityLevel" below - overrides the latter one)]
VarTransform: "N" [List of variable transformations performed before training, e.g.,
"D_Background,P_Signal,G,N_AllClasses" for: "Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for
the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is
assumed)"] H: "False" [Print method-specific help message] CreateMVAPdfs: "True" [Create PDFs for classifier outputs
(signal and background)] TestRate: "5" [Test for overtraining performed at each #th epochs] UseRegulator: "False" [Use
regulator to avoid over-training] # Default: RandomSeed: "1" [Random seed for initial synapse weights (0 means unique
seed for each run; default value '1')] NeuronInputType: "sum" [Neuron input function type] VerbosityLevel: "Default"
[Verbosity level] IgnoreNegWeightsInTraining: "False" [Events with negative weights are ignored in the training (but are
included for testing and performance evaluation)] TrainingMethod: "BP" [Train with Back-Propagation (BP), BFGS Algorithm
(BFGS), or Genetic Algorithm (GA - slower and worse)] LearningRate: "2.000000e-02" [ANN learning rate parameter]
DecayRate: "1.000000e-02" [Decay rate for learning parameter]
EpochMonitoring: "False" [Provide epoch-wise monitoring plots according to TestRate (caution: causes big ROOT output
file!)] Sampling: "1.000000e+00" [Only 'Sampling' (randomly selected) events are trained each epoch] SamplingEpoch:
"1.000000e+00" [Sampling is used for the first 'SamplingEpoch' epochs, afterwards, all events are taken for training]
SamplingImportance: "1.000000e+00" [ The sampling weights of events in epochs which successful (worse estimator than
before) are multiplied with SamplingImportance, else they are divided.] SamplingTraining: "True" [The training sample is
sampled] SamplingTesting: "False" [The testing sample is sampled] ResetStep: "50" [How often BFGS should reset history]
Tau: "3.000000e+00" [LineSearch "size step"]
BPMode: "sequential" [Back-propagation learning mode: sequential or batch]
BatchSize: "-1" [Batch size: number of events/batch, only set if in Batch Mode, -1 for BatchSize=number_of_events]
ConvergenceImprove: "1.000000e-30" [Minimum improvement which counts as improvement (<0 means automatic convergence
check is turned off)] ConvergenceTests: "-1" [Number of steps (without improvement) required for convergence (<0 means
automatic convergence check is turned off)] UpdateLimit: "10000" [Maximum times of regulator update] CalculateErrors:
"False" [Calculates inverse Hessian matrix at the end of the training to be able to calculate the uncertainties of an
MVA value] WeightRange: "1.000000e+00" [Take the events for the estimator calculations from small deviations from the
desired value to large deviations only over the weight range]
##


#VAR -*-*-*-*-*-*-*-*-*-*-*-* variables *-*-*-*-*-*-*-*-*-*-*-*-

NVar 12
UpgradeGhostInfo_obsFT        UpgradeGhostInfo_obsFT        UpgradeGhostInfo_obsFT        UpgradeGhostInfo_obsFT 'F'
[9,12] UpgradeGhostInfo_expFTHitExpectation UpgradeGhostInfo_expFTHitExpectation UpgradeGhostInfo_expFTHitExpectation
UpgradeGhostInfo_expFTHitExpectation                                          'F'    [0,12] UpgradeGhostInfo_FitTChi2
UpgradeGhostInfo_FitTChi2     UpgradeGhostInfo_FitTChi2     UpgradeGhostInfo_FitTChi2 'F'
[0.00109897251241,25.9104213715] UpgradeGhostInfo_FitTNDoF     UpgradeGhostInfo_FitTNDoF     UpgradeGhostInfo_FitTNDoF
UpgradeGhostInfo_FitTNDoF                                       'F'    [2,7] UpgradeGhostInfo_obsUT
UpgradeGhostInfo_obsUT        UpgradeGhostInfo_obsUT        UpgradeGhostInfo_obsUT 'F'    [2,9]
UpgradeGhostInfo_expUTHitExpectation UpgradeGhostInfo_expUTHitExpectation UpgradeGhostInfo_expUTHitExpectation
UpgradeGhostInfo_expUTHitExpectation                                          'F'    [2,10] UpgradeGhostInfo_UToutlier
UpgradeGhostInfo_UToutlier    UpgradeGhostInfo_UToutlier    UpgradeGhostInfo_UToutlier 'F'    [0,2]
UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits 'F'
[119,9416] UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits UpgradeGhostInfo_utHits
'F'    [113,5736] TRACK_CHI2                    TRACK_CHI2                    TRACK_CHI2                    TRACK_CHI2
'F'    [0.0641266554594,38.5001106262] TRACK_PT                      TRACK_PT                      TRACK_PT TRACK_PT 'F'
[0.343108654022,726928.4375] TRACK_ETA                     TRACK_ETA                     TRACK_ETA TRACK_ETA 'F'
[1.45602321625,10.5368804932] NSpec 0


============================================================================ */

#include <array>
#include <cmath>
#include <iostream>
#include <string>
#include <vector>

#ifndef IClassifierReader__def
#  define IClassifierReader__def

class IClassifierReader {

public:
  // constructor
  IClassifierReader() : fStatusIsClean( true ) {}
  virtual ~IClassifierReader() {}

  // return classifier response
  virtual double GetMvaValue( const std::vector<double>& inputValues ) const = 0;

  // returns classifier status
  bool IsStatusClean() const { return fStatusIsClean; }

protected:
  bool fStatusIsClean;
};

#endif

class ReadMLP_5 : public IClassifierReader {

public:
  // constructor
  ReadMLP_5( const std::vector<std::string>& theInputVars )
      : IClassifierReader(), fClassName( "ReadMLP_5" ), fNvars( 12 ), fIsNormalised( false ) {
    // the training input variables
    const char* inputVars[] = {"UpgradeGhostInfo_obsFT",
                               "UpgradeGhostInfo_expFTHitExpectation",
                               "UpgradeGhostInfo_FitTChi2",
                               "UpgradeGhostInfo_FitTNDoF",
                               "UpgradeGhostInfo_obsUT",
                               "UpgradeGhostInfo_expUTHitExpectation",
                               "UpgradeGhostInfo_UToutlier",
                               "UpgradeGhostInfo_veloHits",
                               "UpgradeGhostInfo_utHits",
                               "TRACK_CHI2",
                               "TRACK_PT",
                               "TRACK_ETA"};

    // sanity checks
    if ( theInputVars.size() <= 0 ) {
      std::cout << "Problem in class \"" << fClassName << "\": empty input vector" << std::endl;
      fStatusIsClean = false;
    }

    if ( theInputVars.size() != fNvars ) {
      std::cout << "Problem in class \"" << fClassName
                << "\": mismatch in number of input values: " << theInputVars.size() << " != " << fNvars << std::endl;
      fStatusIsClean = false;
    }

    // validate input variables
    for ( size_t ivar = 0; ivar < theInputVars.size(); ivar++ ) {
      if ( theInputVars[ivar] != inputVars[ivar] ) {
        std::cout << "Problem in class \"" << fClassName << "\": mismatch in input variable names" << std::endl
                  << " for variable [" << ivar << "]: " << theInputVars[ivar].c_str() << " != " << inputVars[ivar]
                  << std::endl;
        fStatusIsClean = false;
      }
    }

    // initialize min and max vectors (for normalisation)
    fVmin[0]  = -1;
    fVmax[0]  = 1;
    fVmin[1]  = -1;
    fVmax[1]  = 1;
    fVmin[2]  = -1;
    fVmax[2]  = 1;
    fVmin[3]  = -1;
    fVmax[3]  = 1;
    fVmin[4]  = -1;
    fVmax[4]  = 1;
    fVmin[5]  = -1;
    fVmax[5]  = 1;
    fVmin[6]  = -1;
    fVmax[6]  = 1;
    fVmin[7]  = -1;
    fVmax[7]  = 1;
    fVmin[8]  = -1;
    fVmax[8]  = 1;
    fVmin[9]  = -1;
    fVmax[9]  = 1;
    fVmin[10] = -1;
    fVmax[10] = 0.99999988079071;
    fVmin[11] = -1;
    fVmax[11] = 1;

    // initialize input variable types
    fType[0]  = 'F';
    fType[1]  = 'F';
    fType[2]  = 'F';
    fType[3]  = 'F';
    fType[4]  = 'F';
    fType[5]  = 'F';
    fType[6]  = 'F';
    fType[7]  = 'F';
    fType[8]  = 'F';
    fType[9]  = 'F';
    fType[10] = 'F';
    fType[11] = 'F';

    // initialize constants
    Initialize();

    // initialize transformation
    InitTransform();
  }

  // the classifier response
  // "inputValues" is a vector of input values in the same order as the
  // variables given to the constructor
  double GetMvaValue( const std::vector<double>& inputValues ) const override;

private:
  // input variable transformation

  double fMin_1[3][12];
  double fMax_1[3][12];
  void   InitTransform_1();
  void   Transform_1( std::vector<double>& iv, int sigOrBgd ) const;
  void   InitTransform();
  void   Transform( std::vector<double>& iv, int sigOrBgd ) const;

  // common member variables
  const char* fClassName;

  const size_t fNvars;
  size_t       GetNvar() const { return fNvars; }
  char         GetType( int ivar ) const { return fType[ivar]; }

  // normalisation of input variables
  const bool fIsNormalised;
  bool       IsNormalised() const { return fIsNormalised; }
  double     fVmin[12];
  double     fVmax[12];
  double     NormVariable( double x, double xmin, double xmax ) const {
    // normalise to output range: [-1, 1]
    return 2 * ( x - xmin ) / ( xmax - xmin ) - 1.0;
  }

  // type of input variable: 'F' or 'I'
  char fType[12];

  // initialize internal variables
  void   Initialize();
  double GetMvaValue__( const std::vector<double>& inputValues ) const;

  // private members (method specific)

  double ActivationFnc( double x ) const;
  double OutputActivationFnc( double x ) const;

  int    fLayers;
  int    fLayerSize[3];
  double fWeightMatrix0to1[18][13]; // weight matrix from layer 0 to 1
  double fWeightMatrix1to2[1][18];  // weight matrix from layer 1 to 2
};

inline void ReadMLP_5::Initialize() {
  // build network structure
  fLayers       = 3;
  fLayerSize[0] = 13;
  fLayerSize[1] = 18;
  fLayerSize[2] = 1;
  // weight matrix from layer 0 to 1
  fWeightMatrix0to1[0][0]   = 1.18433141057908;
  fWeightMatrix0to1[1][0]   = -0.426090414534453;
  fWeightMatrix0to1[2][0]   = -1.34953609424814;
  fWeightMatrix0to1[3][0]   = -1.59167198481109;
  fWeightMatrix0to1[4][0]   = 2.35634539618413;
  fWeightMatrix0to1[5][0]   = -3.31300682272402;
  fWeightMatrix0to1[6][0]   = -1.13227245974276;
  fWeightMatrix0to1[7][0]   = 1.75362569747582;
  fWeightMatrix0to1[8][0]   = -1.41883412647626;
  fWeightMatrix0to1[9][0]   = -2.89552653340885;
  fWeightMatrix0to1[10][0]  = 0.0823362272281397;
  fWeightMatrix0to1[11][0]  = -3.27768046453856;
  fWeightMatrix0to1[12][0]  = -0.356486506242656;
  fWeightMatrix0to1[13][0]  = -2.56579752891205;
  fWeightMatrix0to1[14][0]  = 0.999154494133228;
  fWeightMatrix0to1[15][0]  = 2.1707546002321;
  fWeightMatrix0to1[16][0]  = -2.62286213221471;
  fWeightMatrix0to1[0][1]   = 3.07457021492403;
  fWeightMatrix0to1[1][1]   = 1.55809761462404;
  fWeightMatrix0to1[2][1]   = -1.10197781923428;
  fWeightMatrix0to1[3][1]   = -0.624254488487388;
  fWeightMatrix0to1[4][1]   = -1.28547082919135;
  fWeightMatrix0to1[5][1]   = 1.94494155050658;
  fWeightMatrix0to1[6][1]   = 0.582871450258795;
  fWeightMatrix0to1[7][1]   = 1.1098768344613;
  fWeightMatrix0to1[8][1]   = -0.576302061398135;
  fWeightMatrix0to1[9][1]   = 1.62008343065331;
  fWeightMatrix0to1[10][1]  = -5.13948711649937;
  fWeightMatrix0to1[11][1]  = -3.77743531557635;
  fWeightMatrix0to1[12][1]  = 2.46647379014455;
  fWeightMatrix0to1[13][1]  = -1.05230439678052;
  fWeightMatrix0to1[14][1]  = -1.64408614452312;
  fWeightMatrix0to1[15][1]  = -0.33299314360135;
  fWeightMatrix0to1[16][1]  = 1.3948840934986;
  fWeightMatrix0to1[0][2]   = -0.584558245360136;
  fWeightMatrix0to1[1][2]   = 0.438784844490327;
  fWeightMatrix0to1[2][2]   = -1.93441100554789;
  fWeightMatrix0to1[3][2]   = -1.36784120795516;
  fWeightMatrix0to1[4][2]   = -2.85700050623736;
  fWeightMatrix0to1[5][2]   = 3.03630069275771;
  fWeightMatrix0to1[6][2]   = 3.23157599301255;
  fWeightMatrix0to1[7][2]   = -5.13890190595998;
  fWeightMatrix0to1[8][2]   = 2.36906688316207;
  fWeightMatrix0to1[9][2]   = 4.31382038832387;
  fWeightMatrix0to1[10][2]  = 0.56476095577819;
  fWeightMatrix0to1[11][2]  = -3.37652477565369;
  fWeightMatrix0to1[12][2]  = 2.24294848368354;
  fWeightMatrix0to1[13][2]  = 1.81465794874176;
  fWeightMatrix0to1[14][2]  = -4.87986835860653;
  fWeightMatrix0to1[15][2]  = 3.0856456819623;
  fWeightMatrix0to1[16][2]  = 5.9513884336635;
  fWeightMatrix0to1[0][3]   = -1.17521522904711;
  fWeightMatrix0to1[1][3]   = -0.162617318200079;
  fWeightMatrix0to1[2][3]   = 2.45293923787999;
  fWeightMatrix0to1[3][3]   = 2.49647011488928;
  fWeightMatrix0to1[4][3]   = -2.94061525680444;
  fWeightMatrix0to1[5][3]   = -5.40452960841773;
  fWeightMatrix0to1[6][3]   = 1.70671947385574;
  fWeightMatrix0to1[7][3]   = -3.72797172151957;
  fWeightMatrix0to1[8][3]   = 1.61758346559502;
  fWeightMatrix0to1[9][3]   = 5.60212756893932;
  fWeightMatrix0to1[10][3]  = 0.758285248168758;
  fWeightMatrix0to1[11][3]  = -2.64485829600545;
  fWeightMatrix0to1[12][3]  = -0.186857279921512;
  fWeightMatrix0to1[13][3]  = 4.46975661884063;
  fWeightMatrix0to1[14][3]  = -0.610043107809839;
  fWeightMatrix0to1[15][3]  = -2.86044751377329;
  fWeightMatrix0to1[16][3]  = -1.86678485015025;
  fWeightMatrix0to1[0][4]   = 4.00078879686743;
  fWeightMatrix0to1[1][4]   = -10.1204951670874;
  fWeightMatrix0to1[2][4]   = -0.686138730280917;
  fWeightMatrix0to1[3][4]   = -2.54474757815313;
  fWeightMatrix0to1[4][4]   = 2.56655936052408;
  fWeightMatrix0to1[5][4]   = -0.424713427484899;
  fWeightMatrix0to1[6][4]   = -2.32282441389977;
  fWeightMatrix0to1[7][4]   = -10.6212699065949;
  fWeightMatrix0to1[8][4]   = -18.0171440913425;
  fWeightMatrix0to1[9][4]   = -5.31276215761109;
  fWeightMatrix0to1[10][4]  = 6.36059338111568;
  fWeightMatrix0to1[11][4]  = 0.631107911481535;
  fWeightMatrix0to1[12][4]  = 17.6716015073153;
  fWeightMatrix0to1[13][4]  = 7.28467384282658;
  fWeightMatrix0to1[14][4]  = -4.78823402864019;
  fWeightMatrix0to1[15][4]  = 8.48372159022355;
  fWeightMatrix0to1[16][4]  = -2.22613906864496;
  fWeightMatrix0to1[0][5]   = -1.53926023735395;
  fWeightMatrix0to1[1][5]   = 6.90157352087199;
  fWeightMatrix0to1[2][5]   = -1.58409576041629;
  fWeightMatrix0to1[3][5]   = -1.42268942500876;
  fWeightMatrix0to1[4][5]   = -0.107142389833749;
  fWeightMatrix0to1[5][5]   = 0.268748002394516;
  fWeightMatrix0to1[6][5]   = 1.02172861550452;
  fWeightMatrix0to1[7][5]   = 2.4282665592216;
  fWeightMatrix0to1[8][5]   = -1.34415611327717;
  fWeightMatrix0to1[9][5]   = -1.64740072647596;
  fWeightMatrix0to1[10][5]  = -1.63768599883646;
  fWeightMatrix0to1[11][5]  = 0.638579064683676;
  fWeightMatrix0to1[12][5]  = -8.80777650445035;
  fWeightMatrix0to1[13][5]  = -1.62184190464521;
  fWeightMatrix0to1[14][5]  = 1.24752914762367;
  fWeightMatrix0to1[15][5]  = -3.49548143065488;
  fWeightMatrix0to1[16][5]  = 1.11359961527476;
  fWeightMatrix0to1[0][6]   = -1.55444423901946;
  fWeightMatrix0to1[1][6]   = 10.4961159277498;
  fWeightMatrix0to1[2][6]   = -0.901166189500473;
  fWeightMatrix0to1[3][6]   = 1.00902636213799;
  fWeightMatrix0to1[4][6]   = -0.46965567357685;
  fWeightMatrix0to1[5][6]   = -0.0925666046772324;
  fWeightMatrix0to1[6][6]   = -0.733445095551915;
  fWeightMatrix0to1[7][6]   = 1.41753737500772;
  fWeightMatrix0to1[8][6]   = -0.0899670915902819;
  fWeightMatrix0to1[9][6]   = 0.676117463191031;
  fWeightMatrix0to1[10][6]  = 1.63441292152041;
  fWeightMatrix0to1[11][6]  = 0.0918926149565644;
  fWeightMatrix0to1[12][6]  = -7.1342807941612;
  fWeightMatrix0to1[13][6]  = -0.871312212721207;
  fWeightMatrix0to1[14][6]  = 2.85550570443177;
  fWeightMatrix0to1[15][6]  = -1.48778876198445;
  fWeightMatrix0to1[16][6]  = 0.746107606959696;
  fWeightMatrix0to1[0][7]   = 0.393440681276745;
  fWeightMatrix0to1[1][7]   = -1.25630149902902;
  fWeightMatrix0to1[2][7]   = 0.678502612746484;
  fWeightMatrix0to1[3][7]   = -1.98211447920166;
  fWeightMatrix0to1[4][7]   = 0.500520366248267;
  fWeightMatrix0to1[5][7]   = 0.739117441038015;
  fWeightMatrix0to1[6][7]   = -0.617770282587322;
  fWeightMatrix0to1[7][7]   = 0.442980456709439;
  fWeightMatrix0to1[8][7]   = -0.85670123980929;
  fWeightMatrix0to1[9][7]   = -1.12516787004957;
  fWeightMatrix0to1[10][7]  = 0.160362271288788;
  fWeightMatrix0to1[11][7]  = 1.70330409458083;
  fWeightMatrix0to1[12][7]  = 0.241680123413322;
  fWeightMatrix0to1[13][7]  = -0.575200508304062;
  fWeightMatrix0to1[14][7]  = 0.932502833756614;
  fWeightMatrix0to1[15][7]  = -0.812041039991152;
  fWeightMatrix0to1[16][7]  = 0.350574988621408;
  fWeightMatrix0to1[0][8]   = -1.68012123793744;
  fWeightMatrix0to1[1][8]   = 0.29889296771638;
  fWeightMatrix0to1[2][8]   = 0.0832011185798347;
  fWeightMatrix0to1[3][8]   = -0.971668238645043;
  fWeightMatrix0to1[4][8]   = 0.513466532479351;
  fWeightMatrix0to1[5][8]   = 1.15768294220682;
  fWeightMatrix0to1[6][8]   = -2.94822706736027;
  fWeightMatrix0to1[7][8]   = -0.142753653461606;
  fWeightMatrix0to1[8][8]   = 1.04794105828311;
  fWeightMatrix0to1[9][8]   = -0.0848688049766306;
  fWeightMatrix0to1[10][8]  = 0.464555428163995;
  fWeightMatrix0to1[11][8]  = -1.0075197102037;
  fWeightMatrix0to1[12][8]  = -0.728550132547637;
  fWeightMatrix0to1[13][8]  = -1.70094143284537;
  fWeightMatrix0to1[14][8]  = 0.561911534621687;
  fWeightMatrix0to1[15][8]  = 1.22813117917017;
  fWeightMatrix0to1[16][8]  = 3.07310749127001;
  fWeightMatrix0to1[0][9]   = -0.845959271647825;
  fWeightMatrix0to1[1][9]   = -0.431718981402195;
  fWeightMatrix0to1[2][9]   = -0.593744344901176;
  fWeightMatrix0to1[3][9]   = 2.21140237857178;
  fWeightMatrix0to1[4][9]   = 1.32155528790496;
  fWeightMatrix0to1[5][9]   = -0.245646931378614;
  fWeightMatrix0to1[6][9]   = -4.36838942074924;
  fWeightMatrix0to1[7][9]   = 8.87280794726638;
  fWeightMatrix0to1[8][9]   = -1.28086853604328;
  fWeightMatrix0to1[9][9]   = -3.59097641590608;
  fWeightMatrix0to1[10][9]  = -1.76156752670051;
  fWeightMatrix0to1[11][9]  = 0.89046059180309;
  fWeightMatrix0to1[12][9]  = -2.01936305350295;
  fWeightMatrix0to1[13][9]  = -2.15719559378372;
  fWeightMatrix0to1[14][9]  = 6.65180242351517;
  fWeightMatrix0to1[15][9]  = -10.7779191710722;
  fWeightMatrix0to1[16][9]  = -0.196727726051075;
  fWeightMatrix0to1[0][10]  = -3.40878858827612;
  fWeightMatrix0to1[1][10]  = 0.672272010503172;
  fWeightMatrix0to1[2][10]  = 2.0949209250786;
  fWeightMatrix0to1[3][10]  = 2.79361305019473;
  fWeightMatrix0to1[4][10]  = 0.301756958957167;
  fWeightMatrix0to1[5][10]  = 0.112862616265881;
  fWeightMatrix0to1[6][10]  = 4.99057890793553;
  fWeightMatrix0to1[7][10]  = -1.55623131811123;
  fWeightMatrix0to1[8][10]  = 1.58990725510673;
  fWeightMatrix0to1[9][10]  = 1.56102976152706;
  fWeightMatrix0to1[10][10] = -0.0463563575851017;
  fWeightMatrix0to1[11][10] = 1.69571528835913;
  fWeightMatrix0to1[12][10] = 7.14013467178473;
  fWeightMatrix0to1[13][10] = 5.52173583695824;
  fWeightMatrix0to1[14][10] = -1.06894300688238;
  fWeightMatrix0to1[15][10] = 3.39807364797611;
  fWeightMatrix0to1[16][10] = 1.77680831070775;
  fWeightMatrix0to1[0][11]  = 17.8356927833752;
  fWeightMatrix0to1[1][11]  = -1.39187344077543;
  fWeightMatrix0to1[2][11]  = 1.29767016326919;
  fWeightMatrix0to1[3][11]  = 2.9926981627787;
  fWeightMatrix0to1[4][11]  = 6.98266543606414;
  fWeightMatrix0to1[5][11]  = -0.656688328905408;
  fWeightMatrix0to1[6][11]  = -0.304653628143139;
  fWeightMatrix0to1[7][11]  = 1.90437373623297;
  fWeightMatrix0to1[8][11]  = 7.71454061273819;
  fWeightMatrix0to1[9][11]  = -0.613290435986162;
  fWeightMatrix0to1[10][11] = -2.95011118800132;
  fWeightMatrix0to1[11][11] = -0.13746396771257;
  fWeightMatrix0to1[12][11] = -5.47407194653376;
  fWeightMatrix0to1[13][11] = -15.4643261888812;
  fWeightMatrix0to1[14][11] = 1.40981134096609;
  fWeightMatrix0to1[15][11] = -2.33668669305205;
  fWeightMatrix0to1[16][11] = -3.05208563609896;
  fWeightMatrix0to1[0][12]  = 5.87589987609186;
  fWeightMatrix0to1[1][12]  = -0.664917724657909;
  fWeightMatrix0to1[2][12]  = -2.699060634392;
  fWeightMatrix0to1[3][12]  = -0.904829923084268;
  fWeightMatrix0to1[4][12]  = 1.94579922776992;
  fWeightMatrix0to1[5][12]  = -0.588251473010512;
  fWeightMatrix0to1[6][12]  = -1.62700181838985;
  fWeightMatrix0to1[7][12]  = -1.81831675680824;
  fWeightMatrix0to1[8][12]  = -5.00645301767777;
  fWeightMatrix0to1[9][12]  = -2.03834821537445;
  fWeightMatrix0to1[10][12] = 0.0282516793002797;
  fWeightMatrix0to1[11][12] = 0.758923347120294;
  fWeightMatrix0to1[12][12] = -0.0213246968925924;
  fWeightMatrix0to1[13][12] = -3.12273980831247;
  fWeightMatrix0to1[14][12] = 0.885279957034596;
  fWeightMatrix0to1[15][12] = -2.19180938516183;
  fWeightMatrix0to1[16][12] = 0.208015377352061;
  // weight matrix from layer 1 to 2
  fWeightMatrix1to2[0][0]  = -1.46440268975624;
  fWeightMatrix1to2[0][1]  = -2.19054177509898;
  fWeightMatrix1to2[0][2]  = -1.25564125025915;
  fWeightMatrix1to2[0][3]  = 2.51396220563997;
  fWeightMatrix1to2[0][4]  = -2.48694587122633;
  fWeightMatrix1to2[0][5]  = -1.12671356759368;
  fWeightMatrix1to2[0][6]  = 2.99538831196176;
  fWeightMatrix1to2[0][7]  = -1.64034014661069;
  fWeightMatrix1to2[0][8]  = -1.85429391025151;
  fWeightMatrix1to2[0][9]  = 1.19848594633793;
  fWeightMatrix1to2[0][10] = -3.59266931159004;
  fWeightMatrix1to2[0][11] = 1.61813491194793;
  fWeightMatrix1to2[0][12] = 1.57854283875306;
  fWeightMatrix1to2[0][13] = 1.79129977299462;
  fWeightMatrix1to2[0][14] = -1.08287120805971;
  fWeightMatrix1to2[0][15] = 1.63898190680969;
  fWeightMatrix1to2[0][16] = -4.0112016921918;
  fWeightMatrix1to2[0][17] = 0.41027191128929;
}

inline double ReadMLP_5::GetMvaValue__( const std::vector<double>& inputValues ) const {
  std::array<double, 13> fWeights0{{}};
  std::array<double, 18> fWeights1{{}};
  std::array<double, 1>  fWeights2{{}};

  fWeights0.back() = 1.;
  fWeights1.back() = 1.;

  for ( int i = 0; i < fLayerSize[0] - 1; i++ ) fWeights0[i] = inputValues[i];

  // layer 0 to 1
  for ( int o = 0; o < fLayerSize[1] - 1; o++ ) {
    for ( int i = 0; i < fLayerSize[0]; i++ ) {
      double inputVal = fWeightMatrix0to1[o][i] * fWeights0[i];
      fWeights1[o] += inputVal;
    }
    fWeights1[o] = ActivationFnc( fWeights1[o] );
  }
  // layer 1 to 2
  for ( int o = 0; o < fLayerSize[2]; o++ ) {
    for ( int i = 0; i < fLayerSize[1]; i++ ) {
      double inputVal = fWeightMatrix1to2[o][i] * fWeights1[i];
      fWeights2[o] += inputVal;
    }
    fWeights2[o] = OutputActivationFnc( fWeights2[o] );
  }

  return fWeights2[0];
}

double ReadMLP_5::ActivationFnc( double x ) const {
  // sigmoid
  return 1.0 / ( 1.0 + exp( -x ) );
}
double ReadMLP_5::OutputActivationFnc( double x ) const {
  // sigmoid
  return 1.0 / ( 1.0 + exp( -x ) );
}

inline double ReadMLP_5::GetMvaValue( const std::vector<double>& inputValues ) const {
  // classifier response value
  double retval = 0;

  // classifier response, sanity check first
  if ( !IsStatusClean() ) {
    std::cout << "Problem in class \"" << fClassName << "\": cannot return classifier response"
              << " because status is dirty" << std::endl;
    retval = 0;
  } else {
    if ( IsNormalised() ) {
      // normalise variables
      std::vector<double> iV;
      iV.reserve( inputValues.size() );
      int ivar = 0;
      for ( std::vector<double>::const_iterator varIt = inputValues.begin(); varIt != inputValues.end();
            varIt++, ivar++ ) {
        iV.push_back( NormVariable( *varIt, fVmin[ivar], fVmax[ivar] ) );
      }
      Transform( iV, -1 );
      retval = GetMvaValue__( iV );
    } else {
      std::vector<double> iV;
      int                 ivar = 0;
      for ( std::vector<double>::const_iterator varIt = inputValues.begin(); varIt != inputValues.end();
            varIt++, ivar++ ) {
        iV.push_back( *varIt );
      }
      Transform( iV, -1 );
      retval = GetMvaValue__( iV );
    }
  }

  return retval;
}

//_______________________________________________________________________
inline void ReadMLP_5::InitTransform_1() {
  // Normalization transformation, initialisation
  fMin_1[0][0]  = 9;
  fMax_1[0][0]  = 12;
  fMin_1[1][0]  = 9;
  fMax_1[1][0]  = 12;
  fMin_1[2][0]  = 9;
  fMax_1[2][0]  = 12;
  fMin_1[0][1]  = 4;
  fMax_1[0][1]  = 12;
  fMin_1[1][1]  = 0;
  fMax_1[1][1]  = 12;
  fMin_1[2][1]  = 0;
  fMax_1[2][1]  = 12;
  fMin_1[0][2]  = 0.00523393787444;
  fMax_1[0][2]  = 24.9304523468;
  fMin_1[1][2]  = 0.00109897251241;
  fMax_1[1][2]  = 25.9104213715;
  fMin_1[2][2]  = 0.00109897251241;
  fMax_1[2][2]  = 25.9104213715;
  fMin_1[0][3]  = 2;
  fMax_1[0][3]  = 7;
  fMin_1[1][3]  = 2;
  fMax_1[1][3]  = 7;
  fMin_1[2][3]  = 2;
  fMax_1[2][3]  = 7;
  fMin_1[0][4]  = 2;
  fMax_1[0][4]  = 9;
  fMin_1[1][4]  = 2;
  fMax_1[1][4]  = 8;
  fMin_1[2][4]  = 2;
  fMax_1[2][4]  = 9;
  fMin_1[0][5]  = 2;
  fMax_1[0][5]  = 8;
  fMin_1[1][5]  = 2;
  fMax_1[1][5]  = 10;
  fMin_1[2][5]  = 2;
  fMax_1[2][5]  = 10;
  fMin_1[0][6]  = 0;
  fMax_1[0][6]  = 2;
  fMin_1[1][6]  = 0;
  fMax_1[1][6]  = 2;
  fMin_1[2][6]  = 0;
  fMax_1[2][6]  = 2;
  fMin_1[0][7]  = 119;
  fMax_1[0][7]  = 9416;
  fMin_1[1][7]  = 276;
  fMax_1[1][7]  = 9416;
  fMin_1[2][7]  = 119;
  fMax_1[2][7]  = 9416;
  fMin_1[0][8]  = 113;
  fMax_1[0][8]  = 5736;
  fMin_1[1][8]  = 153;
  fMax_1[1][8]  = 5736;
  fMin_1[2][8]  = 113;
  fMax_1[2][8]  = 5736;
  fMin_1[0][9]  = 0.0641266554594;
  fMax_1[0][9]  = 36.1932640076;
  fMin_1[1][9]  = 0.273953735828;
  fMax_1[1][9]  = 38.5001106262;
  fMin_1[2][9]  = 0.0641266554594;
  fMax_1[2][9]  = 38.5001106262;
  fMin_1[0][10] = 11.6903553009;
  fMax_1[0][10] = 16150.6982422;
  fMin_1[1][10] = 0.343108654022;
  fMax_1[1][10] = 726928.4375;
  fMin_1[2][10] = 0.343108654022;
  fMax_1[2][10] = 726928.4375;
  fMin_1[0][11] = 1.45602321625;
  fMax_1[0][11] = 6.21113300323;
  fMin_1[1][11] = 1.5253534317;
  fMax_1[1][11] = 10.5368804932;
  fMin_1[2][11] = 1.45602321625;
  fMax_1[2][11] = 10.5368804932;
}

//_______________________________________________________________________
inline void ReadMLP_5::Transform_1( std::vector<double>& iv, int cls ) const {
  // Normalization transformation
  if ( cls < 0 || cls > 2 ) {
    if ( 2 > 1 )
      cls = 2;
    else
      cls = 2;
  }
  const int nVar = 12;

  std::array<double, nVar> dv;
  for ( int ivar = 0; ivar < nVar; ivar++ ) dv[ivar] = iv[ivar];
  for ( int ivar = 0; ivar < 12; ivar++ ) {
    double offset = fMin_1[cls][ivar];
    double scale  = 1.0 / ( fMax_1[cls][ivar] - fMin_1[cls][ivar] );
    iv[ivar]      = ( dv[ivar] - offset ) * scale * 2 - 1;
  }
}

//_______________________________________________________________________
inline void ReadMLP_5::InitTransform() { InitTransform_1(); }

//_______________________________________________________________________
inline void ReadMLP_5::Transform( std::vector<double>& iv, int sigOrBgd ) const { Transform_1( iv, sigOrBgd ); }
