/*****************************************************************************\
* (c) Copyright 2000-2018 CERN for the benefit of the LHCb Collaboration      *
*                                                                             *
* This software is distributed under the terms of the GNU General Public      *
* Licence version 3 (GPL Version 3), copied verbatim in the file "COPYING".   *
*                                                                             *
* In applying this licence, CERN does not waive the privileges and immunities *
* granted to it by virtue of its status as an Intergovernmental Organization  *
* or submit itself to any jurisdiction.                                       *
\*****************************************************************************/
// Class: ReadMLP_6
// Automatically generated by MethodBase::MakeClass
//

/* configuration options =====================================================

#GEN -*-*-*-*-*-*-*-*-*-*-*- general info -*-*-*-*-*-*-*-*-*-*-*-

Method         : MLP::MLP_6
TMVA Release   : 4.2.1         [262657]
ROOT Release   : 6.06/00       [394752]
Creator        : hyin
Date           : Thu May 19 09:17:45 2016
Host           : Linux lcgapp-slc6-x86-64-7.cern.ch 2.6.32-573.3.1.el6.x86_64 #1 SMP Fri Aug 14 10:45:09 CEST 2015
x86_64 x86_64 x86_64 GNU/Linux Dir            : /afs/cern.ch/work/h/hyin/workspace/GhostTrack/Upgrade_train/Sigmoid
Training events: 1533771
Analysis type  : [Classification]


#OPT -*-*-*-*-*-*-*-*-*-*-*-*- options -*-*-*-*-*-*-*-*-*-*-*-*-

# Set by User:
NCycles: "600" [Number of training cycles]
HiddenLayers: "N+5" [Specification of hidden layer architecture]
NeuronType: "sigmoid" [Neuron activation function type]
EstimatorType: "CE" [MSE (Mean Square Estimator) for Gaussian Likelihood or CE(Cross-Entropy) for Bernoulli Likelihood]
V: "False" [Verbose output (short form of "VerbosityLevel" below - overrides the latter one)]
VarTransform: "N" [List of variable transformations performed before training, e.g.,
"D_Background,P_Signal,G,N_AllClasses" for: "Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for
the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is
assumed)"] H: "False" [Print method-specific help message] CreateMVAPdfs: "True" [Create PDFs for classifier outputs
(signal and background)] TestRate: "5" [Test for overtraining performed at each #th epochs] UseRegulator: "False" [Use
regulator to avoid over-training] # Default: RandomSeed: "1" [Random seed for initial synapse weights (0 means unique
seed for each run; default value '1')] NeuronInputType: "sum" [Neuron input function type] VerbosityLevel: "Default"
[Verbosity level] IgnoreNegWeightsInTraining: "False" [Events with negative weights are ignored in the training (but are
included for testing and performance evaluation)] TrainingMethod: "BP" [Train with Back-Propagation (BP), BFGS Algorithm
(BFGS), or Genetic Algorithm (GA - slower and worse)] LearningRate: "2.000000e-02" [ANN learning rate parameter]
DecayRate: "1.000000e-02" [Decay rate for learning parameter]
EpochMonitoring: "False" [Provide epoch-wise monitoring plots according to TestRate (caution: causes big ROOT output
file!)] Sampling: "1.000000e+00" [Only 'Sampling' (randomly selected) events are trained each epoch] SamplingEpoch:
"1.000000e+00" [Sampling is used for the first 'SamplingEpoch' epochs, afterwards, all events are taken for training]
SamplingImportance: "1.000000e+00" [ The sampling weights of events in epochs which successful (worse estimator than
before) are multiplied with SamplingImportance, else they are divided.] SamplingTraining: "True" [The training sample is
sampled] SamplingTesting: "False" [The testing sample is sampled] ResetStep: "50" [How often BFGS should reset history]
Tau: "3.000000e+00" [LineSearch "size step"]
BPMode: "sequential" [Back-propagation learning mode: sequential or batch]
BatchSize: "-1" [Batch size: number of events/batch, only set if in Batch Mode, -1 for BatchSize=number_of_events]
ConvergenceImprove: "1.000000e-30" [Minimum improvement which counts as improvement (<0 means automatic convergence
check is turned off)] ConvergenceTests: "-1" [Number of steps (without improvement) required for convergence (<0 means
automatic convergence check is turned off)] UpdateLimit: "10000" [Maximum times of regulator update] CalculateErrors:
"False" [Calculates inverse Hessian matrix at the end of the training to be able to calculate the uncertainties of an
MVA value] WeightRange: "1.000000e+00" [Take the events for the estimator calculations from small deviations from the
desired value to large deviations only over the weight range]
##


#VAR -*-*-*-*-*-*-*-*-*-*-*-* variables *-*-*-*-*-*-*-*-*-*-*-*-

NVar 10
UpgradeGhostInfo_obsFT        UpgradeGhostInfo_obsFT        UpgradeGhostInfo_obsFT        UpgradeGhostInfo_obsFT 'F'
[9,12] UpgradeGhostInfo_expFTHitExpectation UpgradeGhostInfo_expFTHitExpectation UpgradeGhostInfo_expFTHitExpectation
UpgradeGhostInfo_expFTHitExpectation                                          'F'    [0,12] UpgradeGhostInfo_FitTChi2
UpgradeGhostInfo_FitTChi2     UpgradeGhostInfo_FitTChi2     UpgradeGhostInfo_FitTChi2 'F'
[0.00239633303136,1364.74145508] UpgradeGhostInfo_FitTNDoF     UpgradeGhostInfo_FitTNDoF     UpgradeGhostInfo_FitTNDoF
UpgradeGhostInfo_FitTNDoF                                       'F'    [2,7] UpgradeGhostInfo_veloHits
UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits     UpgradeGhostInfo_veloHits 'F'    [124,9416]
UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits       UpgradeGhostInfo_utHits 'F'
[113,5736] TRACK_CHI2                    TRACK_CHI2                    TRACK_CHI2                    TRACK_CHI2 'F'
[0.0016328189522,1356.86315918] TRACK_NDOF                    TRACK_NDOF                    TRACK_NDOF TRACK_NDOF 'F'
[2,7] TRACK_PT                      TRACK_PT                      TRACK_PT                      TRACK_PT 'F'
[1.68443369865,99635704] TRACK_ETA                     TRACK_ETA                     TRACK_ETA TRACK_ETA 'F'
[0.511984407902,9.62973117828] NSpec 0


============================================================================ */

#include <array>
#include <cmath>
#include <iostream>
#include <string>
#include <vector>

#ifndef IClassifierReader__def
#  define IClassifierReader__def

class IClassifierReader {

public:
  // constructor
  IClassifierReader() : fStatusIsClean( true ) {}
  virtual ~IClassifierReader() {}

  // return classifier response
  virtual double GetMvaValue( const std::vector<double>& inputValues ) const = 0;

  // returns classifier status
  bool IsStatusClean() const { return fStatusIsClean; }

protected:
  bool fStatusIsClean;
};

#endif

class ReadMLP_6 : public IClassifierReader {

public:
  // constructor
  ReadMLP_6( const std::vector<std::string>& theInputVars )
      : IClassifierReader(), fClassName( "ReadMLP_6" ), fNvars( 10 ), fIsNormalised( false ) {
    // the training input variables
    const char* inputVars[] = {"UpgradeGhostInfo_obsFT",
                               "UpgradeGhostInfo_expFTHitExpectation",
                               "UpgradeGhostInfo_FitTChi2",
                               "UpgradeGhostInfo_FitTNDoF",
                               "UpgradeGhostInfo_veloHits",
                               "UpgradeGhostInfo_utHits",
                               "TRACK_CHI2",
                               "TRACK_NDOF",
                               "TRACK_PT",
                               "TRACK_ETA"};

    // sanity checks
    if ( theInputVars.size() <= 0 ) {
      std::cout << "Problem in class \"" << fClassName << "\": empty input vector" << std::endl;
      fStatusIsClean = false;
    }

    if ( theInputVars.size() != fNvars ) {
      std::cout << "Problem in class \"" << fClassName
                << "\": mismatch in number of input values: " << theInputVars.size() << " != " << fNvars << std::endl;
      fStatusIsClean = false;
    }

    // validate input variables
    for ( size_t ivar = 0; ivar < theInputVars.size(); ivar++ ) {
      if ( theInputVars[ivar] != inputVars[ivar] ) {
        std::cout << "Problem in class \"" << fClassName << "\": mismatch in input variable names" << std::endl
                  << " for variable [" << ivar << "]: " << theInputVars[ivar].c_str() << " != " << inputVars[ivar]
                  << std::endl;
        fStatusIsClean = false;
      }
    }

    // initialize min and max vectors (for normalisation)
    fVmin[0] = -1;
    fVmax[0] = 1;
    fVmin[1] = -1;
    fVmax[1] = 1;
    fVmin[2] = -1;
    fVmax[2] = 1;
    fVmin[3] = -1;
    fVmax[3] = 1;
    fVmin[4] = -1;
    fVmax[4] = 1;
    fVmin[5] = -1;
    fVmax[5] = 1;
    fVmin[6] = -1;
    fVmax[6] = 1;
    fVmin[7] = -1;
    fVmax[7] = 1;
    fVmin[8] = -1;
    fVmax[8] = 1;
    fVmin[9] = -1;
    fVmax[9] = 1;

    // initialize input variable types
    fType[0] = 'F';
    fType[1] = 'F';
    fType[2] = 'F';
    fType[3] = 'F';
    fType[4] = 'F';
    fType[5] = 'F';
    fType[6] = 'F';
    fType[7] = 'F';
    fType[8] = 'F';
    fType[9] = 'F';

    // initialize constants
    Initialize();

    // initialize transformation
    InitTransform();
  }

  // the classifier response
  // "inputValues" is a vector of input values in the same order as the
  // variables given to the constructor
  double GetMvaValue( const std::vector<double>& inputValues ) const override;

private:
  // input variable transformation

  double fMin_1[3][10];
  double fMax_1[3][10];
  void   InitTransform_1();
  void   Transform_1( std::vector<double>& iv, int sigOrBgd ) const;
  void   InitTransform();
  void   Transform( std::vector<double>& iv, int sigOrBgd ) const;

  // common member variables
  const char* fClassName;

  const size_t fNvars;
  size_t       GetNvar() const { return fNvars; }
  char         GetType( int ivar ) const { return fType[ivar]; }

  // normalisation of input variables
  const bool fIsNormalised;
  bool       IsNormalised() const { return fIsNormalised; }
  double     fVmin[10];
  double     fVmax[10];
  double     NormVariable( double x, double xmin, double xmax ) const {
    // normalise to output range: [-1, 1]
    return 2 * ( x - xmin ) / ( xmax - xmin ) - 1.0;
  }

  // type of input variable: 'F' or 'I'
  char fType[10];

  // initialize internal variables
  void   Initialize();
  double GetMvaValue__( const std::vector<double>& inputValues ) const;

  // private members (method specific)

  double ActivationFnc( double x ) const;
  double OutputActivationFnc( double x ) const;

  int    fLayers;
  int    fLayerSize[3];
  double fWeightMatrix0to1[16][11]; // weight matrix from layer 0 to 1
  double fWeightMatrix1to2[1][16];  // weight matrix from layer 1 to 2
};

inline void ReadMLP_6::Initialize() {
  // build network structure
  fLayers       = 3;
  fLayerSize[0] = 11;
  fLayerSize[1] = 16;
  fLayerSize[2] = 1;
  // weight matrix from layer 0 to 1
  fWeightMatrix0to1[0][0]   = -6.14265114370991;
  fWeightMatrix0to1[1][0]   = 0.782526830951874;
  fWeightMatrix0to1[2][0]   = -0.799591466951973;
  fWeightMatrix0to1[3][0]   = 5.40206560758498;
  fWeightMatrix0to1[4][0]   = 2.69269857623229;
  fWeightMatrix0to1[5][0]   = 0.0966251591751369;
  fWeightMatrix0to1[6][0]   = -1.09285441820192;
  fWeightMatrix0to1[7][0]   = 1.55820483929823;
  fWeightMatrix0to1[8][0]   = -6.71847351571026;
  fWeightMatrix0to1[9][0]   = -0.493768587645009;
  fWeightMatrix0to1[10][0]  = -3.1099136498699;
  fWeightMatrix0to1[11][0]  = -0.554439025699484;
  fWeightMatrix0to1[12][0]  = -6.3605834193352;
  fWeightMatrix0to1[13][0]  = 0.004365597060669;
  fWeightMatrix0to1[14][0]  = -0.551633746492037;
  fWeightMatrix0to1[0][1]   = 5.75558227075701;
  fWeightMatrix0to1[1][1]   = 0.0604617813765483;
  fWeightMatrix0to1[2][1]   = 11.5096231354534;
  fWeightMatrix0to1[3][1]   = -0.343536153015893;
  fWeightMatrix0to1[4][1]   = 8.40477627840155;
  fWeightMatrix0to1[5][1]   = -0.755654441268499;
  fWeightMatrix0to1[6][1]   = -4.67425897162071;
  fWeightMatrix0to1[7][1]   = -4.86473177857564;
  fWeightMatrix0to1[8][1]   = -5.85644382556328;
  fWeightMatrix0to1[9][1]   = -0.836822156723263;
  fWeightMatrix0to1[10][1]  = -2.8083826644983;
  fWeightMatrix0to1[11][1]  = 9.56389039893609;
  fWeightMatrix0to1[12][1]  = -3.48169068949599;
  fWeightMatrix0to1[13][1]  = -24.0909305219031;
  fWeightMatrix0to1[14][1]  = 5.40471900879018;
  fWeightMatrix0to1[0][2]   = 3.2806189551488;
  fWeightMatrix0to1[1][2]   = -17.4202907299577;
  fWeightMatrix0to1[2][2]   = -10.2902798015332;
  fWeightMatrix0to1[3][2]   = 7.85731075442327;
  fWeightMatrix0to1[4][2]   = 14.7951253197585;
  fWeightMatrix0to1[5][2]   = -1.3993518103806;
  fWeightMatrix0to1[6][2]   = -64.1070462772519;
  fWeightMatrix0to1[7][2]   = 22.3529753007921;
  fWeightMatrix0to1[8][2]   = -3.43836760613513;
  fWeightMatrix0to1[9][2]   = 7.46204824208316;
  fWeightMatrix0to1[10][2]  = 33.2453232767234;
  fWeightMatrix0to1[11][2]  = -6.78965415595365;
  fWeightMatrix0to1[12][2]  = 3.80367594062836;
  fWeightMatrix0to1[13][2]  = -50.5705548368045;
  fWeightMatrix0to1[14][2]  = 33.7641345648473;
  fWeightMatrix0to1[0][3]   = -3.12452696012045;
  fWeightMatrix0to1[1][3]   = 1.47395943411248;
  fWeightMatrix0to1[2][3]   = 0.534112785860924;
  fWeightMatrix0to1[3][3]   = 1.19010341264757;
  fWeightMatrix0to1[4][3]   = 0.147459407338848;
  fWeightMatrix0to1[5][3]   = 2.22460200914124;
  fWeightMatrix0to1[6][3]   = 1.879327972309;
  fWeightMatrix0to1[7][3]   = -1.45750569508328;
  fWeightMatrix0to1[8][3]   = -0.445284980288343;
  fWeightMatrix0to1[9][3]   = -0.482390716817223;
  fWeightMatrix0to1[10][3]  = -2.87228469789866;
  fWeightMatrix0to1[11][3]  = -1.30042231180521;
  fWeightMatrix0to1[12][3]  = -1.87127497477928;
  fWeightMatrix0to1[13][3]  = 0.914050723675941;
  fWeightMatrix0to1[14][3]  = -0.663915912444891;
  fWeightMatrix0to1[0][4]   = 0.0702096241425197;
  fWeightMatrix0to1[1][4]   = -0.564898638125325;
  fWeightMatrix0to1[2][4]   = -0.351222113323285;
  fWeightMatrix0to1[3][4]   = -0.0919374488766413;
  fWeightMatrix0to1[4][4]   = 1.36365261503567;
  fWeightMatrix0to1[5][4]   = 0.0133471651338636;
  fWeightMatrix0to1[6][4]   = 0.130527900533931;
  fWeightMatrix0to1[7][4]   = 0.137838693610656;
  fWeightMatrix0to1[8][4]   = 0.0923832093181882;
  fWeightMatrix0to1[9][4]   = -0.0226061260906502;
  fWeightMatrix0to1[10][4]  = -0.188749027706813;
  fWeightMatrix0to1[11][4]  = -0.598703882061029;
  fWeightMatrix0to1[12][4]  = 0.0782590992556993;
  fWeightMatrix0to1[13][4]  = -0.47622223694346;
  fWeightMatrix0to1[14][4]  = 0.348342686625341;
  fWeightMatrix0to1[0][5]   = -4.44884783848494;
  fWeightMatrix0to1[1][5]   = -0.599798913618393;
  fWeightMatrix0to1[2][5]   = 0.464483804042139;
  fWeightMatrix0to1[3][5]   = -0.687469272592947;
  fWeightMatrix0to1[4][5]   = -0.668955256738564;
  fWeightMatrix0to1[5][5]   = 0.504914838210286;
  fWeightMatrix0to1[6][5]   = 0.0376143300118089;
  fWeightMatrix0to1[7][5]   = 1.53799404551288;
  fWeightMatrix0to1[8][5]   = -2.00299756836903;
  fWeightMatrix0to1[9][5]   = -0.420877191630536;
  fWeightMatrix0to1[10][5]  = 1.44328116112673;
  fWeightMatrix0to1[11][5]  = 0.170850891801919;
  fWeightMatrix0to1[12][5]  = 1.92685855932207;
  fWeightMatrix0to1[13][5]  = -2.18709124085883;
  fWeightMatrix0to1[14][5]  = 3.05117336095261;
  fWeightMatrix0to1[0][6]   = 2.3839892685439;
  fWeightMatrix0to1[1][6]   = -19.4459513500972;
  fWeightMatrix0to1[2][6]   = -8.4797218112962;
  fWeightMatrix0to1[3][6]   = 6.49184200113462;
  fWeightMatrix0to1[4][6]   = 13.9207493057543;
  fWeightMatrix0to1[5][6]   = 0.489151659443765;
  fWeightMatrix0to1[6][6]   = -64.3438195912214;
  fWeightMatrix0to1[7][6]   = 19.2784177282904;
  fWeightMatrix0to1[8][6]   = -3.70321153797628;
  fWeightMatrix0to1[9][6]   = 6.4562004717211;
  fWeightMatrix0to1[10][6]  = 30.5453205058881;
  fWeightMatrix0to1[11][6]  = -9.43324179486664;
  fWeightMatrix0to1[12][6]  = 2.68810031943232;
  fWeightMatrix0to1[13][6]  = -50.6476648198634;
  fWeightMatrix0to1[14][6]  = 33.7988773696468;
  fWeightMatrix0to1[0][7]   = 0.14606331819403;
  fWeightMatrix0to1[1][7]   = -0.233144403169372;
  fWeightMatrix0to1[2][7]   = -0.342776963569386;
  fWeightMatrix0to1[3][7]   = -0.349160530563606;
  fWeightMatrix0to1[4][7]   = -0.975065078584379;
  fWeightMatrix0to1[5][7]   = -1.14037446866911;
  fWeightMatrix0to1[6][7]   = 0.826388443712099;
  fWeightMatrix0to1[7][7]   = 0.498787903663885;
  fWeightMatrix0to1[8][7]   = -1.30794516571963;
  fWeightMatrix0to1[9][7]   = -0.0516955648737991;
  fWeightMatrix0to1[10][7]  = -0.138341972206794;
  fWeightMatrix0to1[11][7]  = 0.377478146884224;
  fWeightMatrix0to1[12][7]  = -1.06433183757901;
  fWeightMatrix0to1[13][7]  = 0.200514152894131;
  fWeightMatrix0to1[14][7]  = -2.42196639143729;
  fWeightMatrix0to1[0][8]   = -1.21347229111114;
  fWeightMatrix0to1[1][8]   = 6.98510263517127;
  fWeightMatrix0to1[2][8]   = 11.7861943422093;
  fWeightMatrix0to1[3][8]   = 0.416771152998645;
  fWeightMatrix0to1[4][8]   = -8.49630880087376;
  fWeightMatrix0to1[5][8]   = 1.42379379124709;
  fWeightMatrix0to1[6][8]   = 42.0606618114006;
  fWeightMatrix0to1[7][8]   = -15.259505601974;
  fWeightMatrix0to1[8][8]   = -2.14727433825083;
  fWeightMatrix0to1[9][8]   = -4.17084455209908;
  fWeightMatrix0to1[10][8]  = -18.8051932165361;
  fWeightMatrix0to1[11][8]  = 6.40103841047336;
  fWeightMatrix0to1[12][8]  = 9.94641637062185;
  fWeightMatrix0to1[13][8]  = 30.0978633969611;
  fWeightMatrix0to1[14][8]  = -17.3929828670621;
  fWeightMatrix0to1[0][9]   = 2.66789896730946;
  fWeightMatrix0to1[1][9]   = 14.4654815871783;
  fWeightMatrix0to1[2][9]   = -1.17543593939812;
  fWeightMatrix0to1[3][9]   = -14.5138885051981;
  fWeightMatrix0to1[4][9]   = 11.0686437801729;
  fWeightMatrix0to1[5][9]   = 9.71040737634913;
  fWeightMatrix0to1[6][9]   = -0.308236270654461;
  fWeightMatrix0to1[7][9]   = -4.71285388335013;
  fWeightMatrix0to1[8][9]   = 0.949597797182453;
  fWeightMatrix0to1[9][9]   = -35.2003963122932;
  fWeightMatrix0to1[10][9]  = -2.24085885159926;
  fWeightMatrix0to1[11][9]  = 3.57993871146324;
  fWeightMatrix0to1[12][9]  = -9.63057883272283;
  fWeightMatrix0to1[13][9]  = -1.78195834821405;
  fWeightMatrix0to1[14][9]  = 0.379339022383565;
  fWeightMatrix0to1[0][10]  = -8.94578797365799;
  fWeightMatrix0to1[1][10]  = -24.1850596088423;
  fWeightMatrix0to1[2][10]  = -15.2976676341186;
  fWeightMatrix0to1[3][10]  = 5.76393281011656;
  fWeightMatrix0to1[4][10]  = 17.871546969876;
  fWeightMatrix0to1[5][10]  = 1.11781758921923;
  fWeightMatrix0to1[6][10]  = -82.9289989866347;
  fWeightMatrix0to1[7][10]  = 24.4472852083947;
  fWeightMatrix0to1[8][10]  = 4.9611555527969;
  fWeightMatrix0to1[9][10]  = -20.4186980962349;
  fWeightMatrix0to1[10][10] = 46.5686574446023;
  fWeightMatrix0to1[11][10] = -12.0508112839686;
  fWeightMatrix0to1[12][10] = 5.3268777614291;
  fWeightMatrix0to1[13][10] = -48.2139480131199;
  fWeightMatrix0to1[14][10] = 47.2234369228139;
  // weight matrix from layer 1 to 2
  fWeightMatrix1to2[0][0]  = 2.33547141739177;
  fWeightMatrix1to2[0][1]  = 1.86407889161705;
  fWeightMatrix1to2[0][2]  = 5.16644581647596;
  fWeightMatrix1to2[0][3]  = -1.16018912581871;
  fWeightMatrix1to2[0][4]  = -1.69623413486834;
  fWeightMatrix1to2[0][5]  = -2.11710692733878;
  fWeightMatrix1to2[0][6]  = 7.68395836404152;
  fWeightMatrix1to2[0][7]  = -4.79340647978824;
  fWeightMatrix1to2[0][8]  = -2.93666572249599;
  fWeightMatrix1to2[0][9]  = 7.84717123992136;
  fWeightMatrix1to2[0][10] = -3.29176499962065;
  fWeightMatrix1to2[0][11] = 5.41061995395517;
  fWeightMatrix1to2[0][12] = -3.78832456999173;
  fWeightMatrix1to2[0][13] = 1.48591131879462;
  fWeightMatrix1to2[0][14] = -4.62164427979332;
  fWeightMatrix1to2[0][15] = -5.1517396423052;
}

inline double ReadMLP_6::GetMvaValue__( const std::vector<double>& inputValues ) const {
  std::array<double, 11> fWeights0{{}};
  std::array<double, 16> fWeights1{{}};
  std::array<double, 1>  fWeights2{{}};

  fWeights0.back() = 1.;
  fWeights1.back() = 1.;

  for ( int i = 0; i < fLayerSize[0] - 1; i++ ) fWeights0[i] = inputValues[i];

  // layer 0 to 1
  for ( int o = 0; o < fLayerSize[1] - 1; o++ ) {
    for ( int i = 0; i < fLayerSize[0]; i++ ) {
      double inputVal = fWeightMatrix0to1[o][i] * fWeights0[i];
      fWeights1[o] += inputVal;
    }
    fWeights1[o] = ActivationFnc( fWeights1[o] );
  }
  // layer 1 to 2
  for ( int o = 0; o < fLayerSize[2]; o++ ) {
    for ( int i = 0; i < fLayerSize[1]; i++ ) {
      double inputVal = fWeightMatrix1to2[o][i] * fWeights1[i];
      fWeights2[o] += inputVal;
    }
    fWeights2[o] = OutputActivationFnc( fWeights2[o] );
  }

  return fWeights2[0];
}

double ReadMLP_6::ActivationFnc( double x ) const {
  // sigmoid
  return 1.0 / ( 1.0 + exp( -x ) );
}
double ReadMLP_6::OutputActivationFnc( double x ) const {
  // sigmoid
  return 1.0 / ( 1.0 + exp( -x ) );
}

inline double ReadMLP_6::GetMvaValue( const std::vector<double>& inputValues ) const {
  // classifier response value
  double retval = 0;

  // classifier response, sanity check first
  if ( !IsStatusClean() ) {
    std::cout << "Problem in class \"" << fClassName << "\": cannot return classifier response"
              << " because status is dirty" << std::endl;
    retval = 0;
  } else {
    if ( IsNormalised() ) {
      // normalise variables
      std::vector<double> iV;
      iV.reserve( inputValues.size() );
      int ivar = 0;
      for ( std::vector<double>::const_iterator varIt = inputValues.begin(); varIt != inputValues.end();
            varIt++, ivar++ ) {
        iV.push_back( NormVariable( *varIt, fVmin[ivar], fVmax[ivar] ) );
      }
      Transform( iV, -1 );
      retval = GetMvaValue__( iV );
    } else {
      std::vector<double> iV;
      int                 ivar = 0;
      for ( std::vector<double>::const_iterator varIt = inputValues.begin(); varIt != inputValues.end();
            varIt++, ivar++ ) {
        iV.push_back( *varIt );
      }
      Transform( iV, -1 );
      retval = GetMvaValue__( iV );
    }
  }

  return retval;
}

//_______________________________________________________________________
inline void ReadMLP_6::InitTransform_1() {
  // Normalization transformation, initialisation
  fMin_1[0][0] = 9;
  fMax_1[0][0] = 12;
  fMin_1[1][0] = 9;
  fMax_1[1][0] = 12;
  fMin_1[2][0] = 9;
  fMax_1[2][0] = 12;
  fMin_1[0][1] = 4;
  fMax_1[0][1] = 12;
  fMin_1[1][1] = 0;
  fMax_1[1][1] = 12;
  fMin_1[2][1] = 0;
  fMax_1[2][1] = 12;
  fMin_1[0][2] = 0.00460837129503;
  fMax_1[0][2] = 121.001655579;
  fMin_1[1][2] = 0.00239633303136;
  fMax_1[1][2] = 1364.74145508;
  fMin_1[2][2] = 0.00239633303136;
  fMax_1[2][2] = 1364.74145508;
  fMin_1[0][3] = 2;
  fMax_1[0][3] = 7;
  fMin_1[1][3] = 2;
  fMax_1[1][3] = 7;
  fMin_1[2][3] = 2;
  fMax_1[2][3] = 7;
  fMin_1[0][4] = 124;
  fMax_1[0][4] = 9416;
  fMin_1[1][4] = 206;
  fMax_1[1][4] = 9416;
  fMin_1[2][4] = 124;
  fMax_1[2][4] = 9416;
  fMin_1[0][5] = 113;
  fMax_1[0][5] = 5736;
  fMin_1[1][5] = 155;
  fMax_1[1][5] = 5736;
  fMin_1[2][5] = 113;
  fMax_1[2][5] = 5736;
  fMin_1[0][6] = 0.00458535552025;
  fMax_1[0][6] = 120.876304626;
  fMin_1[1][6] = 0.0016328189522;
  fMax_1[1][6] = 1356.86315918;
  fMin_1[2][6] = 0.0016328189522;
  fMax_1[2][6] = 1356.86315918;
  fMin_1[0][7] = 2;
  fMax_1[0][7] = 7;
  fMin_1[1][7] = 2;
  fMax_1[1][7] = 7;
  fMin_1[2][7] = 2;
  fMax_1[2][7] = 7;
  fMin_1[0][8] = 1.68443369865;
  fMax_1[0][8] = 14334913;
  fMin_1[1][8] = 2.82720208168;
  fMax_1[1][8] = 99635704;
  fMin_1[2][8] = 1.68443369865;
  fMax_1[2][8] = 99635704;
  fMin_1[0][9] = 0.806472361088;
  fMax_1[0][9] = 9.62973117828;
  fMin_1[1][9] = 0.511984407902;
  fMax_1[1][9] = 8.67352104187;
  fMin_1[2][9] = 0.511984407902;
  fMax_1[2][9] = 9.62973117828;
}

//_______________________________________________________________________
inline void ReadMLP_6::Transform_1( std::vector<double>& iv, int cls ) const {
  // Normalization transformation
  if ( cls < 0 || cls > 2 ) {
    if ( 2 > 1 )
      cls = 2;
    else
      cls = 2;
  }
  const int nVar = 10;

  std::array<double, nVar> dv;
  for ( int ivar = 0; ivar < nVar; ivar++ ) dv[ivar] = iv[ivar];
  for ( int ivar = 0; ivar < 10; ivar++ ) {
    double offset = fMin_1[cls][ivar];
    double scale  = 1.0 / ( fMax_1[cls][ivar] - fMin_1[cls][ivar] );
    iv[ivar]      = ( dv[ivar] - offset ) * scale * 2 - 1;
  }
}

//_______________________________________________________________________
inline void ReadMLP_6::InitTransform() { InitTransform_1(); }

//_______________________________________________________________________
inline void ReadMLP_6::Transform( std::vector<double>& iv, int sigOrBgd ) const { Transform_1( iv, sigOrBgd ); }
